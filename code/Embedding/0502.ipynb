{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------\n",
    "# ライブラリ\n",
    "# ----------\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from psutil import virtual_memory\n",
    "\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import xgboost as xgb\n",
    "import scipy.stats as stats\n",
    "import lightgbm as lgbm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import fontstyle\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# DIR = \"/content/drive/MyDrive/Competitions/probspace/研究論文の国際学会採択予測\"\n",
    "DIR = '/workspace'\n",
    "INPUT_DIR = os.path.join(DIR,\"input\")\n",
    "OUTPUT_DIR = os.path.join(DIR,\"output\")\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------\n",
    "# 設定\n",
    "# ----------\n",
    "num_fold = 5\n",
    "seed = 0\n",
    "\n",
    "DEVICE = \"cuda\" # \"cpu\" or \"cuda\"\n",
    "tokenizer = None\n",
    "BATCH_SIZE = 16\n",
    "MAX_LEN = 768\n",
    "\n",
    "# テキスト特徴として連結するカラム\n",
    "txt_columns = ['title', 'keywords', 'abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------\n",
    "# 関数\n",
    "# ----------\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        current_device = torch.cuda.current_device()\n",
    "        print(\"Device:\", torch.cuda.get_device_name(current_device))\n",
    "        ram_gb = virtual_memory().total / 1e9\n",
    "        print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "def get_stratifiedkfold(train, target_col, n_splits, seed):\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    generator = kf.split(train.to_numpy(), train.get_column(target_col).to_numpy())\n",
    "    fold_array = np.zeros(len(train))\n",
    "    for fold, (_, idx_valid) in enumerate(generator):\n",
    "        fold_array[idx_valid] = fold\n",
    "    return fold_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習用にtitle, abstract, keywordsの要素数を特徴として追加しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: NVIDIA RTX A4000\n",
      "Your runtime has 16.6 gigabytes of available RAM\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 11)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>title</th><th>year</th><th>abstract</th><th>keywords</th><th>y</th><th>txt_feat</th><th>num_title</th><th>num_abstract</th><th>num_keywords</th><th>group</th></tr><tr><td>i64</td><td>str</td><td>i64</td><td>str</td><td>str</td><td>i64</td><td>str</td><td>u32</td><td>u32</td><td>u32</td><td>str</td></tr></thead><tbody><tr><td>1</td><td>&quot;Hierarchical A…</td><td>2018</td><td>&quot;We propose a n…</td><td>&quot;generative, hi…</td><td>0</td><td>&quot;Hierarchical A…</td><td>4</td><td>155</td><td>7</td><td>&quot;2018-0&quot;</td></tr><tr><td>2</td><td>&quot;Learning to Co…</td><td>2018</td><td>&quot;Words in natur…</td><td>&quot;NLU, word embe…</td><td>0</td><td>&quot;Learning to Co…</td><td>8</td><td>130</td><td>5</td><td>&quot;2018-0&quot;</td></tr><tr><td>3</td><td>&quot;Graph2Seq: Sca…</td><td>2018</td><td>&quot;Neural network…</td><td>&quot;&quot;</td><td>0</td><td>&quot;Graph2Seq: Sca…</td><td>6</td><td>143</td><td>0</td><td>&quot;2018-0&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 11)\n",
       "┌─────┬───────────────┬──────┬──────────────┬───┬───────────┬──────────────┬──────────────┬────────┐\n",
       "│ id  ┆ title         ┆ year ┆ abstract     ┆ … ┆ num_title ┆ num_abstract ┆ num_keywords ┆ group  │\n",
       "│ --- ┆ ---           ┆ ---  ┆ ---          ┆   ┆ ---       ┆ ---          ┆ ---          ┆ ---    │\n",
       "│ i64 ┆ str           ┆ i64  ┆ str          ┆   ┆ u32       ┆ u32          ┆ u32          ┆ str    │\n",
       "╞═════╪═══════════════╪══════╪══════════════╪═══╪═══════════╪══════════════╪══════════════╪════════╡\n",
       "│ 1   ┆ Hierarchical  ┆ 2018 ┆ We propose a ┆ … ┆ 4         ┆ 155          ┆ 7            ┆ 2018-0 │\n",
       "│     ┆ Adversarially ┆      ┆ novel        ┆   ┆           ┆              ┆              ┆        │\n",
       "│     ┆ Learn…        ┆      ┆ hierarchical ┆   ┆           ┆              ┆              ┆        │\n",
       "│     ┆               ┆      ┆ …            ┆   ┆           ┆              ┆              ┆        │\n",
       "│ 2   ┆ Learning to   ┆ 2018 ┆ Words in     ┆ … ┆ 8         ┆ 130          ┆ 5            ┆ 2018-0 │\n",
       "│     ┆ Compute Word  ┆      ┆ natural      ┆   ┆           ┆              ┆              ┆        │\n",
       "│     ┆ Embeddi…      ┆      ┆ language     ┆   ┆           ┆              ┆              ┆        │\n",
       "│     ┆               ┆      ┆ follow…      ┆   ┆           ┆              ┆              ┆        │\n",
       "│ 3   ┆ Graph2Seq:    ┆ 2018 ┆ Neural       ┆ … ┆ 6         ┆ 143          ┆ 0            ┆ 2018-0 │\n",
       "│     ┆ Scalable      ┆      ┆ networks are ┆   ┆           ┆              ┆              ┆        │\n",
       "│     ┆ Learning Dyn… ┆      ┆ increasingly ┆   ┆           ┆              ┆              ┆        │\n",
       "│     ┆               ┆      ┆ …            ┆   ┆           ┆              ┆              ┆        │\n",
       "└─────┴───────────────┴──────┴──────────────┴───┴───────────┴──────────────┴──────────────┴────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>title</th><th>year</th><th>abstract</th><th>keywords</th><th>txt_feat</th><th>num_title</th><th>num_abstract</th><th>num_keywords</th></tr><tr><td>i64</td><td>str</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>u32</td><td>u32</td><td>u32</td></tr></thead><tbody><tr><td>1</td><td>&quot;StyleAlign: An…</td><td>2022</td><td>&quot;In this paper,…</td><td>&quot;StyleGAN, tran…</td><td>&quot;StyleAlign: An…</td><td>8</td><td>209</td><td>11</td></tr><tr><td>2</td><td>&quot;Embedding a ra…</td><td>2021</td><td>&quot;We develop a t…</td><td>&quot;Graph neural n…</td><td>&quot;Embedding a ra…</td><td>16</td><td>272</td><td>11</td></tr><tr><td>3</td><td>&quot;BBRefinement: …</td><td>2021</td><td>&quot;We present a c…</td><td>&quot;object detecti…</td><td>&quot;BBRefinement: …</td><td>11</td><td>152</td><td>6</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 9)\n",
       "┌─────┬──────────────┬──────┬──────────────┬───┬─────────────┬───────────┬────────────┬────────────┐\n",
       "│ id  ┆ title        ┆ year ┆ abstract     ┆ … ┆ txt_feat    ┆ num_title ┆ num_abstra ┆ num_keywor │\n",
       "│ --- ┆ ---          ┆ ---  ┆ ---          ┆   ┆ ---         ┆ ---       ┆ ct         ┆ ds         │\n",
       "│ i64 ┆ str          ┆ i64  ┆ str          ┆   ┆ str         ┆ u32       ┆ ---        ┆ ---        │\n",
       "│     ┆              ┆      ┆              ┆   ┆             ┆           ┆ u32        ┆ u32        │\n",
       "╞═════╪══════════════╪══════╪══════════════╪═══╪═════════════╪═══════════╪════════════╪════════════╡\n",
       "│ 1   ┆ StyleAlign:  ┆ 2022 ┆ In this      ┆ … ┆ StyleAlign: ┆ 8         ┆ 209        ┆ 11         │\n",
       "│     ┆ Analysis and ┆      ┆ paper, we    ┆   ┆ Analysis    ┆           ┆            ┆            │\n",
       "│     ┆ Applica…     ┆      ┆ perform an   ┆   ┆ and         ┆           ┆            ┆            │\n",
       "│     ┆              ┆      ┆ in-…         ┆   ┆ Applica…    ┆           ┆            ┆            │\n",
       "│ 2   ┆ Embedding a  ┆ 2021 ┆ We develop a ┆ … ┆ Embedding a ┆ 16        ┆ 272        ┆ 11         │\n",
       "│     ┆ random graph ┆      ┆ theory for   ┆   ┆ random      ┆           ┆            ┆            │\n",
       "│     ┆ via GNN…     ┆      ┆ embeddin…    ┆   ┆ graph via   ┆           ┆            ┆            │\n",
       "│     ┆              ┆      ┆              ┆   ┆ GNN…        ┆           ┆            ┆            │\n",
       "│ 3   ┆ BBRefinement ┆ 2021 ┆ We present a ┆ … ┆ BBRefinemen ┆ 11        ┆ 152        ┆ 6          │\n",
       "│     ┆ : an         ┆      ┆ conceptually ┆   ┆ t: an       ┆           ┆            ┆            │\n",
       "│     ┆ universal    ┆      ┆ simple…      ┆   ┆ universal   ┆           ┆            ┆            │\n",
       "│     ┆ schem…       ┆      ┆              ┆   ┆ schem…      ┆           ┆            ┆            │\n",
       "└─────┴──────────────┴──────┴──────────────┴───┴─────────────┴───────────┴────────────┴────────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ----------\n",
    "# データ\n",
    "# ----------\n",
    "train = pl.read_csv(os.path.join(INPUT_DIR,'train_data.csv'))\n",
    "test = pl.read_csv(os.path.join(INPUT_DIR,'test_data.csv'))\n",
    "sub = pl.read_csv(os.path.join(INPUT_DIR,'submission.csv'))\n",
    "\n",
    "# ----------\n",
    "# 前処理・特徴生成\n",
    "# ----------\n",
    "set_seed(seed)\n",
    "\n",
    "# テキスト特徴の作成\n",
    "# グループごとにFold数を設定\n",
    "train =\\\n",
    "train.with_columns(\n",
    "    pl.concat_str(txt_columns, separator='_').alias('txt_feat'),\n",
    "    # title\n",
    "    pl.when(pl.col('title').str.to_lowercase().is_in(['', 'nan', '0', 'blank']))\n",
    "    .then(0)\n",
    "    .otherwise(pl.col('title').str.to_lowercase().str.count_match(' ') + 1)\n",
    "    .alias('num_title'),\n",
    "    # abstract\n",
    "    pl.when(pl.col('abstract').str.to_lowercase().is_in(['', 'nan', '0', 'blank']))\n",
    "    .then(0)\n",
    "    .otherwise(pl.col('abstract').str.to_lowercase().str.count_match(' ') + 1)\n",
    "    .alias('num_abstract'),\n",
    "    # keywords\n",
    "    pl.when(pl.col('keywords').str.to_lowercase().is_in(['', 'nan', '0', 'blank']))\n",
    "    .then(0)\n",
    "    .otherwise(pl.col('keywords').str.to_lowercase().str.count_match(' ') + 1)\n",
    "    .alias('num_keywords'),\n",
    "    # group\n",
    "    pl.concat_str(['year', 'y'], separator='-').alias('group'),\n",
    "    )\n",
    "\n",
    "test = \\\n",
    "test.with_columns(\n",
    "    pl.concat_str(txt_columns, separator='. ').alias('txt_feat'),\n",
    "    # title\n",
    "    pl.when(pl.col('title').str.to_lowercase().is_in(['', 'nan', '0', 'blank']))\n",
    "    .then(0)\n",
    "    .otherwise(pl.col('title').str.to_lowercase().str.count_match(' ') + 1)\n",
    "    .alias('num_title'),\n",
    "    # abstract\n",
    "    pl.when(pl.col('abstract').str.to_lowercase().is_in(['', 'nan', '0', 'blank']))\n",
    "    .then(0)\n",
    "    .otherwise(pl.col('abstract').str.to_lowercase().str.count_match(' ') + 1)\n",
    "    .alias('num_abstract'),\n",
    "    # keywords\n",
    "    pl.when(pl.col('keywords').str.to_lowercase().is_in(['', 'nan', '0', 'blank']))\n",
    "    .then(0)\n",
    "    .otherwise(pl.col('keywords').str.to_lowercase().str.count_match(' ') + 1)\n",
    "    .alias('num_keywords'),\n",
    "    )\n",
    "\n",
    "display(train.head(3))\n",
    "display(test.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------\n",
    "# BERT\n",
    "# ----------\n",
    "\n",
    "# Dataset\n",
    "class EmbedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df[idx, 'txt_feat']\n",
    "        tokens = tokenizer(\n",
    "                text,\n",
    "                None,\n",
    "                add_special_tokens=True,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=MAX_LEN,\n",
    "                return_tensors=\"pt\")\n",
    "        tokens = {k:v.squeeze(0) for k,v in tokens.items()}\n",
    "        return tokens\n",
    "\n",
    "# Pooling\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state.detach().cpu()\n",
    "    input_mask_expanded = (\n",
    "        attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    )\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n",
    "        input_mask_expanded.sum(1), min=1e-9\n",
    "    )\n",
    "\n",
    "def get_embeddings(MODEL_NM='', MAX_LEN=512, BATCH_SIZE=4, verbose=True):\n",
    "    global tokenizer, DEVICE\n",
    "\n",
    "    model = AutoModel.from_pretrained(MODEL_NM)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NM)\n",
    "    \n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "        \n",
    "    # train\n",
    "    all_train_text_feats = []\n",
    "    for batch in tqdm(embed_dataloader_tr,total=len(embed_dataloader_tr)):\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            model_output = model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        sentence_embeddings = mean_pooling(model_output, attention_mask.detach().cpu())\n",
    "        # Normalize the embeddings\n",
    "        sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "        sentence_embeddings =  sentence_embeddings.squeeze(0).detach().cpu().numpy()\n",
    "        all_train_text_feats.extend(sentence_embeddings)\n",
    "    all_train_text_feats = np.array(all_train_text_feats)\n",
    "    if verbose:\n",
    "        print('Train embeddings shape',all_train_text_feats.shape)\n",
    "    \n",
    "    # test\n",
    "    te_text_feats = []\n",
    "    for batch in tqdm(embed_dataloader_te,total=len(embed_dataloader_te)):\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            model_output = model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        sentence_embeddings = mean_pooling(model_output, attention_mask.detach().cpu())\n",
    "        # Normalize the embeddings\n",
    "        sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "        sentence_embeddings =  sentence_embeddings.squeeze(0).detach().cpu().numpy()\n",
    "        te_text_feats.extend(sentence_embeddings)\n",
    "    te_text_feats = np.array(te_text_feats)\n",
    "    if verbose:\n",
    "        print('Test embeddings shape',te_text_feats.shape)\n",
    "        \n",
    "    # save feat\n",
    "    np.save(f\"{MODEL_NM.split('/')[-1]}_train\", all_train_text_feats)\n",
    "    np.save(f\"{MODEL_NM.split('/')[-1]}_test\", te_text_feats)\n",
    "\n",
    "    return all_train_text_feats, te_text_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_tr = EmbedDataset(train)\n",
    "embed_dataloader_tr = torch.utils.data.DataLoader(ds_tr,\\\n",
    "                        batch_size=BATCH_SIZE,\\\n",
    "                        shuffle=False)\n",
    "ds_te = EmbedDataset(test)\n",
    "embed_dataloader_te = torch.utils.data.DataLoader(ds_te,\\\n",
    "                        batch_size=BATCH_SIZE,\\\n",
    "                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4998804db8ed4cc685e1218f71563777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train embeddings shape (4974, 768)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66da6020068d49eeb27703381703a575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test embeddings shape (6393, 768)\n",
      "CPU times: user 10min 2s, sys: 15.3 s, total: 10min 18s\n",
      "Wall time: 9min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "MODEL_NM = 'microsoft/deberta-v3-base'\n",
    "train_emb, test_emb = get_embeddings(MODEL_NM, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "emb_col = [f'emb{i}' for i in range(train_emb.shape[1])]\n",
    "train_emb_df = pl.DataFrame(train_emb, schema=emb_col)\n",
    "train = pl.concat([train, train_emb_df], how='horizontal')\n",
    "test_emb_df = pl.DataFrame(test_emb, schema=emb_col)\n",
    "test = pl.concat([test, test_emb_df], how='horizontal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[30m\u001b[1m< Seed : 0 >\u001b[0m\n",
      "Device: NVIDIA RTX A4000\n",
      "Your runtime has 16.6 gigabytes of available RAM\n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: gamma\n",
      "[LightGBM] [Warning] lambda_l2 is set with reg_lambda=0.0, will be overridden by lambda=1. Current value: lambda_l2=1\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Fatal] Check failed: (alpha) > (0.0) at /__w/1/s/python-package/compile/src/io/config_auto.cpp, line 584 .\n",
      "\n"
     ]
    },
    {
     "ename": "LightGBMError",
     "evalue": "Check failed: (alpha) > (0.0) at /__w/1/s/python-package/compile/src/io/config_auto.cpp, line 584 .\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLightGBMError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 40\u001b[0m\n\u001b[1;32m     23\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary:logistic\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m10000\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtree_method\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpu_hist\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     37\u001b[0m }\n\u001b[1;32m     39\u001b[0m clf \u001b[38;5;241m=\u001b[39m lgbm\u001b[38;5;241m.\u001b[39mLGBMClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m---> 40\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtr_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtr_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mva_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mva_y\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m va_preds_p \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict_proba(va_x)[:, \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     47\u001b[0m oof_preds[\n\u001b[1;32m     48\u001b[0m     train\u001b[38;5;241m.\u001b[39mselect(\n\u001b[1;32m     49\u001b[0m         pl\u001b[38;5;241m.\u001b[39mwhen(pl\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfolds\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m==\u001b[39mfold)\u001b[38;5;241m.\u001b[39mthen(\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39motherwise(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     50\u001b[0m         )\u001b[38;5;241m.\u001b[39mto_numpy()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m     51\u001b[0m         ] \u001b[38;5;241m=\u001b[39m va_preds_p\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.9/lib/python3.10/site-packages/lightgbm/sklearn.py:967\u001b[0m, in \u001b[0;36mLGBMClassifier.fit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    965\u001b[0m             valid_sets[i] \u001b[38;5;241m=\u001b[39m (valid_x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_le\u001b[38;5;241m.\u001b[39mtransform(valid_y))\n\u001b[0;32m--> 967\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_class_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_class_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.9/lib/python3.10/site-packages/lightgbm/sklearn.py:748\u001b[0m, in \u001b[0;36mLGBMModel.fit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    745\u001b[0m evals_result \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    746\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mappend(record_evaluation(evals_result))\n\u001b[0;32m--> 748\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metrics_callable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evals_result:\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evals_result \u001b[38;5;241m=\u001b[39m evals_result\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.9/lib/python3.10/site-packages/lightgbm/engine.py:271\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# construct booster\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 271\u001b[0m     booster \u001b[38;5;241m=\u001b[39m \u001b[43mBooster\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_valid_contain_train:\n\u001b[1;32m    273\u001b[0m         booster\u001b[38;5;241m.\u001b[39mset_train_data_name(train_data_name)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.9/lib/python3.10/site-packages/lightgbm/basic.py:2605\u001b[0m, in \u001b[0;36mBooster.__init__\u001b[0;34m(self, params, train_set, model_file, model_str, silent)\u001b[0m\n\u001b[1;32m   2598\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_network(\n\u001b[1;32m   2599\u001b[0m         machines\u001b[38;5;241m=\u001b[39mmachines,\n\u001b[1;32m   2600\u001b[0m         local_listen_port\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal_listen_port\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   2601\u001b[0m         listen_time_out\u001b[38;5;241m=\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_out\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m120\u001b[39m),\n\u001b[1;32m   2602\u001b[0m         num_machines\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_machines\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   2603\u001b[0m     )\n\u001b[1;32m   2604\u001b[0m \u001b[38;5;66;03m# construct booster object\u001b[39;00m\n\u001b[0;32m-> 2605\u001b[0m \u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2606\u001b[0m \u001b[38;5;66;03m# copy the parameters from train_set\u001b[39;00m\n\u001b[1;32m   2607\u001b[0m params\u001b[38;5;241m.\u001b[39mupdate(train_set\u001b[38;5;241m.\u001b[39mget_params())\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.9/lib/python3.10/site-packages/lightgbm/basic.py:1815\u001b[0m, in \u001b[0;36mDataset.construct\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1812\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_init_score_by_predictor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predictor, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, used_indices)\n\u001b[1;32m   1813\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1814\u001b[0m     \u001b[38;5;66;03m# create train\u001b[39;00m\n\u001b[0;32m-> 1815\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1816\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1817\u001b[0m \u001b[43m                    \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predictor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1818\u001b[0m \u001b[43m                    \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1819\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1820\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfree_raw_data:\n\u001b[1;32m   1821\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.9/lib/python3.10/site-packages/lightgbm/basic.py:1538\u001b[0m, in \u001b[0;36mDataset._lazy_init\u001b[0;34m(self, data, label, reference, weight, group, init_score, predictor, silent, feature_name, categorical_feature, params)\u001b[0m\n\u001b[1;32m   1536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__init_from_csc(data, params_str, ref_dataset)\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m-> 1538\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__init_from_np2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1540\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.9/lib/python3.10/site-packages/lightgbm/basic.py:1659\u001b[0m, in \u001b[0;36mDataset.__init_from_np2d\u001b[0;34m(self, mat, params_str, ref_dataset)\u001b[0m\n\u001b[1;32m   1656\u001b[0m     data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(mat\u001b[38;5;241m.\u001b[39mreshape(mat\u001b[38;5;241m.\u001b[39msize), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m   1658\u001b[0m ptr_data, type_ptr_data, _ \u001b[38;5;241m=\u001b[39m c_float_array(data)\n\u001b[0;32m-> 1659\u001b[0m \u001b[43m_safe_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLGBM_DatasetCreateFromMat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1660\u001b[0m \u001b[43m    \u001b[49m\u001b[43mptr_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1661\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtype_ptr_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int32\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int32\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mC_API_IS_ROW_MAJOR\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mc_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_str\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mref_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.9/lib/python3.10/site-packages/lightgbm/basic.py:125\u001b[0m, in \u001b[0;36m_safe_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value from C API call.\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03m    The return value from C API calls.\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(_LIB\u001b[38;5;241m.\u001b[39mLGBM_GetLastError()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mLightGBMError\u001b[0m: Check failed: (alpha) > (0.0) at /__w/1/s/python-package/compile/src/io/config_auto.cpp, line 584 .\n"
     ]
    }
   ],
   "source": [
    "# Run XGBoost\n",
    "use_col = ['num_title', 'num_abstract', 'num_keywords'] + emb_col\n",
    "test_x = test.select(use_col).to_numpy()\n",
    "\n",
    "whole_va_preds = []\n",
    "whole_test_preds = []\n",
    "for seed in range(3):\n",
    "    print(fontstyle.apply(f'< Seed : {seed} >', 'BLACK/BOLD'))\n",
    "    set_seed(seed)\n",
    "    train = train.with_columns(\n",
    "        pl.Series(get_stratifiedkfold(train, 'group', num_fold, seed))\n",
    "        .alias('folds')\n",
    "        )\n",
    "    \n",
    "    oof_preds = np.zeros((len(train), ), dtype=np.float32)\n",
    "    preds = []\n",
    "    for fold in range(num_fold):\n",
    "        tr_x = train.filter(pl.col('folds')!=fold).select(use_col).to_numpy()\n",
    "        tr_y = train.filter(pl.col('folds')!=fold).select('y').to_numpy()\n",
    "        va_x = train.filter(pl.col('folds')==fold).select(use_col).to_numpy()\n",
    "        va_y = train.filter(pl.col('folds')==fold).select('y').to_numpy()\n",
    "\n",
    "        params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'n_estimators': 10000,\n",
    "        'random_state': 0, \n",
    "        'learning_rate': 0.01,\n",
    "        'max_depth': 8,\n",
    "        'colsample_bytree': 1.0,\n",
    "        'colsample_bylevel': 0.5,\n",
    "        'subsample': 0.9,\n",
    "        'gamma': 0,\n",
    "        'lambda': 1,\n",
    "        'alpha': 0,\n",
    "        'min_child_weight': 1,\n",
    "        'tree_method': 'gpu_hist',\n",
    "        }\n",
    "\n",
    "        clf = xgb.XGBClassifier(**params)\n",
    "        clf.fit(\n",
    "            tr_x, tr_y,\n",
    "            eval_set=[(va_x, va_y)],\n",
    "            early_stopping_rounds=100,\n",
    "            verbose=100)\n",
    "\n",
    "        va_preds_p = clf.predict_proba(va_x)[:, 1]\n",
    "        oof_preds[\n",
    "            train.select(\n",
    "                pl.when(pl.col('folds')==fold).then(True).otherwise(False)\n",
    "                ).to_numpy().reshape(-1).astype(bool)\n",
    "                ] = va_preds_p\n",
    "        va_preds = (va_preds_p > 0.5).astype(int)\n",
    "        score = accuracy_score(va_y, va_preds)\n",
    "        print(f'Fold : {fold+1} Accuracy score: {score}')\n",
    "        print()\n",
    "        test_preds_p = clf.predict_proba(test_x)[:, 1]\n",
    "        preds.append(test_preds_p)\n",
    "\n",
    "    score_s = accuracy_score(train.select('y').to_numpy(), oof_preds > 0.5)\n",
    "    print(fontstyle.apply(f'Seed{seed} Accuracy score : {score_s}', 'BLACK/BOLD'))\n",
    "    print()\n",
    "    whole_va_preds.append(oof_preds)\n",
    "    whole_test_preds.append(preds)\n",
    "\n",
    "# preds_va_p = np.mean(whole_va_preds, axis=0)\n",
    "# whole_score = accuracy_score(train.select('y').to_numpy(), preds_va_p > 0.5)\n",
    "# preds_test = (np.mean(np.mean(whole_test_preds, axis=0), axis=0) > 0.5).astype(int)\n",
    "preds_va = np.array([np.where(preds > 0.5, 1, 0) for preds in whole_va_preds])\n",
    "whole_score = accuracy_score(train.select('y').to_numpy(), stats.mode(preds_va, axis=0).mode.flatten())\n",
    "test_preds_array = np.array(whole_test_preds)\n",
    "test_preds_array = test_preds_array.reshape(test_preds_array.shape[0]*test_preds_array.shape[1], -1)\n",
    "preds_test = np.array([np.where(preds > 0.5, 1, 0) for preds in test_preds_array])\n",
    "preds_test = stats.mode(preds_test, axis=0).mode.flatten()\n",
    "print()  \n",
    "print(fontstyle.apply(f'whole Accuracy score: {whole_score}', 'BLACK/BOLD'))\n",
    "print()\n",
    "\n",
    "display(pl.Series(preds_test).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
