{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f30a1e2-96c1-4a32-8521-0cae1e758c91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold,StratifiedGroupKFold,GroupKFold\n",
    "from sklearn.metrics import log_loss,f1_score,accuracy_score\n",
    "\n",
    "from transformers import AutoModel, AutoConfig, AutoTokenizer, AdamW, DataCollatorWithPadding\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35b3bcf0-a4a9-4073-8b9e-7fbe51d6e1f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "INPUT_DIR = '/workspace/data/'\n",
    "OUTPUT_DIR = '/workspace/data/output/'\n",
    "OUTPUT_SUB_DIR = os.path.join(OUTPUT_DIR,'Submission')\n",
    "OUTPUT_MODEL_DIR = os.path.join('/workspace/model','microsoft-deberta-v3-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "338f1ee5-b44a-426f-90b0-10d21bc1aba8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    num_workers=2\n",
    "    path=OUTPUT_MODEL_DIR\n",
    "    config_path=OUTPUT_MODEL_DIR+'config.pth'\n",
    "    model=\"microsoft/deberta-v3-large\"\n",
    "    batch_size=16\n",
    "    dropout=0.1\n",
    "    target_size=4\n",
    "    max_len=1024\n",
    "    seed=42\n",
    "    n_fold=5\n",
    "    trn_fold=[0, 1, 2, 3, 4]\n",
    "    gradient_checkpointing=True\n",
    "    freezing=True\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c83f843e-82fe-4af4-88ab-c9f85e953770",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loss Func\n",
    "def criterion(outputs, labels):\n",
    "    return nn.CrossEntropyLoss()(outputs, labels)\n",
    "\n",
    "def softmax(z):\n",
    "    assert len(z.shape) == 2\n",
    "    s = np.max(z, axis=1)\n",
    "    s = s[:, np.newaxis] # necessary step to do broadcasting\n",
    "    e_x = np.exp(z - s)\n",
    "    div = np.sum(e_x, axis=1)\n",
    "    div = div[:, np.newaxis] # dito\n",
    "    return e_x / div\n",
    "\"\"\"\n",
    "def get_score(y_true, y_pred):\n",
    "    y_pred = softmax(y_pred)\n",
    "    score = log_loss(y_true, y_pred)\n",
    "    return round(score, 5)\n",
    "\"\"\"\n",
    "def get_score(outputs, labels):\n",
    "    outputs = F.softmax(torch.tensor(outputs)).numpy()\n",
    "    return accuracy_score(np.argmax(outputs,axis=1),labels)\n",
    "\n",
    "def get_logger(filename=OUTPUT_DIR+'train'):\n",
    "    from logging import getLogger, INFO, FileHandler, Formatter, StreamHandler\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = get_logger()\n",
    "\n",
    "def seed_everything(seed=CFG.seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54c73b80-bf8c-41e3-89b2-38c0585327d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CV Score: 0.7023\n"
     ]
    }
   ],
   "source": [
    "\n",
    "oof_df = pd.read_pickle(OUTPUT_MODEL_DIR + '/'+ CFG.model.replace('/','-')+'oof_df.pkl')\n",
    "labels = oof_df['y'].values\n",
    "preds = oof_df[['0','1']].values.tolist()\n",
    "score = get_score(preds, labels)\n",
    "LOGGER.info(f'CV Score: {score:<.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f03514a1-ebb3-4f26-8053-629931320573",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def freeze(module):\n",
    "    \"\"\"\n",
    "    Freezes module's parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    for parameter in module.parameters():\n",
    "        parameter.requires_grad = False\n",
    "        \n",
    "def get_freezed_parameters(module):\n",
    "    \"\"\"\n",
    "    Returns names of freezed parameters of the given module.\n",
    "    \"\"\"\n",
    "    \n",
    "    freezed_parameters = []\n",
    "    for name, parameter in module.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            freezed_parameters.append(name)\n",
    "            \n",
    "    return freezed_parameters\n",
    "\n",
    "def set_embedding_parameters_bits(embeddings_path, optim_bits=32):\n",
    "    \"\"\"\n",
    "    https://github.com/huggingface/transformers/issues/14819#issuecomment-1003427930\n",
    "    \"\"\"\n",
    "    \n",
    "    embedding_types = (\"word\", \"position\", \"token_type\")\n",
    "    for embedding_type in embedding_types:\n",
    "        attr_name = f\"{embedding_type}_embeddings\"\n",
    "        \n",
    "        if hasattr(embeddings_path, attr_name): \n",
    "            bnb.optim.GlobalOptimManager.get_instance().register_module_override(\n",
    "                getattr(embeddings_path, attr_name), 'weight', {'optim_bits': optim_bits}\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "439298df-816c-469c-a96b-de4a0c96847e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(os.path.join(INPUT_DIR, 'test_data.csv'))\n",
    "submission_df = pd.read_csv(os.path.join(INPUT_DIR, 'submission.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a30cffd-6dff-41fb-adb7-723e6d4fccb4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model)\n",
    "CFG.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcf20995-57a5-4b7d-aa36-d449aee30695",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length):\n",
    "        self.df = df\n",
    "        self.max_len = CFG.max_len\n",
    "        self.text = df['abstract'].values\n",
    "        self.tokenizer = CFG.tokenizer\n",
    "        #self.targets = df['label'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.text[index]\n",
    "        inputs = tokenizer.encode_plus(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length = self.max_len\n",
    "        )\n",
    "        samples = {\n",
    "            'input_ids': inputs['input_ids'],\n",
    "            'attention_mask': inputs['attention_mask'],\n",
    "            #'target': self.targets[index]\n",
    "        }\n",
    "\n",
    "        if 'token_type_ids' in inputs:\n",
    "            samples['token_type_ids'] = inputs['token_type_ids']\n",
    "            \n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abd53461-cbec-4778-a953-645686c49f51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class Collate:\n",
    "    def __init__(self, tokenizer, isTrain=True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.isTrain = isTrain\n",
    "        # self.args = args\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        output = dict()\n",
    "        output[\"input_ids\"] = [sample[\"input_ids\"] for sample in batch]\n",
    "        output[\"attention_mask\"] = [sample[\"attention_mask\"] for sample in batch]\n",
    "        if self.isTrain:\n",
    "            output[\"target\"] = [sample[\"target\"] for sample in batch]\n",
    "\n",
    "        # calculate max token length of this batch\n",
    "        batch_max = max([len(ids) for ids in output[\"input_ids\"]])\n",
    "\n",
    "        # add padding\n",
    "        if self.tokenizer.padding_side == \"right\":\n",
    "            output[\"input_ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"input_ids\"]]\n",
    "            output[\"attention_mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"attention_mask\"]]\n",
    "        else:\n",
    "            output[\"input_ids\"] = [(batch_max - len(s)) * [self.tokenizer.pad_token_id] + s for s in output[\"input_ids\"]]\n",
    "            output[\"attention_mask\"] = [(batch_max - len(s)) * [0] + s for s in output[\"attention_mask\"]]\n",
    "\n",
    "        # convert to tensors\n",
    "        output[\"input_ids\"] = torch.tensor(output[\"input_ids\"], dtype=torch.long)\n",
    "        output[\"attention_mask\"] = torch.tensor(output[\"attention_mask\"], dtype=torch.long)\n",
    "        if self.isTrain:\n",
    "            output[\"target\"] = torch.tensor(output[\"target\"], dtype=torch.long)\n",
    "\n",
    "        return output\n",
    "    \n",
    "collate_fn = Collate(CFG.tokenizer, isTrain=False)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de171267-57be-42c3-97ab-0b9252625f01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "        \n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9) #\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7de514d-b054-4b08-83c9-b604df8a444a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(CustomModel, self).__init__()\n",
    "        # Header (fast or normal)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Gradient_checkpointing\n",
    "        if CFG.gradient_checkpointing:\n",
    "            (self.model).gradient_checkpointing_enable()\n",
    "        \n",
    "        # Freezing\n",
    "        if CFG.freezing:\n",
    "            # freezing embeddings and first 2 layers of encoder\n",
    "            freeze((self.model).embeddings)\n",
    "            freeze((self.model).encoder.layer[:2])\n",
    "            CFG.after_freezed_parameters = filter(lambda parameter: parameter.requires_grad, (self.model).parameters())\n",
    "        \n",
    "        self.config = AutoConfig.from_pretrained(model_name)\n",
    "        self.drop = nn.Dropout(p=CFG.dropout)\n",
    "        self.pooler = MeanPooling()\n",
    "        self.fc = nn.Linear(self.config.hidden_size, CFG.target_size)\n",
    "        \n",
    "    def forward(self, ids, mask):        \n",
    "        out = self.model(input_ids=ids, \n",
    "                         attention_mask=mask,\n",
    "                         output_hidden_states=False)\n",
    "        out = self.pooler(out.last_hidden_state, mask)\n",
    "        out = self.drop(out)\n",
    "        outputs = self.fc(out)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c36e5206-d882-4fdb-914b-b183731326af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def inference_one_epoch(model, dataloader, device):\n",
    "    model.eval()\n",
    "    pred = []\n",
    "    model.to(device)\n",
    "    for step, data in enumerate(dataloader):\n",
    "        ids = data['input_ids'].to(device, dtype=torch.long)\n",
    "        mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(ids, mask)\n",
    "        pred.append(outputs.to('cpu').numpy())\n",
    "    pred = np.concatenate(pred)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "222579c8-489f-48a1-9ad0-1371c8a9334b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                     | 0/5 [00:00<?, ?it/s]Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      " 20%|████████████                                                | 1/5 [05:04<20:17, 304.32s/it]Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      " 40%|████████████████████████                                    | 2/5 [10:07<15:10, 303.65s/it]Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      " 60%|████████████████████████████████████                        | 3/5 [15:13<10:09, 304.79s/it]Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      " 80%|████████████████████████████████████████████████            | 4/5 [20:23<05:06, 306.85s/it]Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|████████████████████████████████████████████████████████████| 5/5 [25:33<00:00, 306.73s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "testdataset = Dataset(test, CFG.tokenizer, CFG.max_len)\n",
    "\n",
    "test_loader = DataLoader(testdataset, \n",
    "                         batch_size=CFG.batch_size,\n",
    "                         shuffle=False,\n",
    "                         collate_fn = collate_fn,\n",
    "                         num_workers = CFG.num_workers,\n",
    "                         pin_memory = True,\n",
    "                         drop_last = False,\n",
    "                         )\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for fold in tqdm(CFG.trn_fold):\n",
    "    model = CustomModel(CFG.model)\n",
    "    config_path=CFG.config_path\n",
    "    state = torch.load(OUTPUT_MODEL_DIR + '/'+ CFG.model.replace('/','-')+f\"_fold{fold}_best.pth\",\n",
    "                       map_location = torch.device('cuda'))\n",
    "    model.load_state_dict(state['model'])\n",
    "\n",
    "    prediction = inference_one_epoch(model, test_loader, device)\n",
    "    prediction = F.softmax(torch.tensor(prediction)).numpy().astype(float)\n",
    "    predictions.append(prediction)\n",
    "    del model, state, prediction\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1cde2cf-a341-4810-92d8-79454c981fa9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission_df.to_csv(os.path.join(OUTPUT_MODEL_DIR,'sub.csv'),index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4dead3d2-639a-4512-a449-f38fe1d6ec09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6388</th>\n",
       "      <td>6389</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6389</th>\n",
       "      <td>6390</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6390</th>\n",
       "      <td>6391</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6391</th>\n",
       "      <td>6392</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6392</th>\n",
       "      <td>6393</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6393 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  y\n",
       "0        1  0\n",
       "1        2  0\n",
       "2        3  0\n",
       "3        4  1\n",
       "4        5  0\n",
       "...    ... ..\n",
       "6388  6389  1\n",
       "6389  6390  0\n",
       "6390  6391  0\n",
       "6391  6392  0\n",
       "6392  6393  0\n",
       "\n",
       "[6393 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(os.path.join(OUTPUT_MODEL_DIR,'sub.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
