{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "82acfafb-1b7d-4bb5-9316-e50213a6bdb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold,StratifiedGroupKFold,GroupKFold,train_test_split\n",
    "from sklearn.metrics import log_loss,f1_score,accuracy_score\n",
    "\n",
    "from transformers import AutoModel, AutoConfig, AutoTokenizer, AdamW, DataCollatorWithPadding\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# 各種パスの設定\n",
    "######################################################################\n",
    "base_path = '/workspace' # ベースとなるパスを指定してください。#######\n",
    "######################################################################\n",
    "os.makedirs(os.path.join(base_path,'model'), exist_ok=True)  # 学習済みモデルの保存するディレクトリを作成\n",
    "os.makedirs(os.path.join(base_path,'output'), exist_ok=True)  # 提出用ファイルを出力するディレクトリを作成\n",
    "train_data_path = os.path.join(base_path,'data/train_data.csv') # 訓練データのパスを指定\n",
    "test_data_path = os.path.join(base_path,'data/test_data.csv') # テストデータのパスを指定\n",
    "submit_data_path = os.path.join(base_path,'data/submission.csv') # 提出用サンプルfileのパスを指定\n",
    "model_file_path = os.path.join(base_path,'model') # 学習済みモデルのパスを指定\n",
    "output_file_path = os.path.join(base_path,'output/20230303_submission.csv') # 提出用ファイルのパスを指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51aae901-4fa0-4e54-921d-d95735f363f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    wandb = False\n",
    "    apex = True\n",
    "    model = 'microsoft/deberta-v3-large'\n",
    "    seed = 42\n",
    "    n_splits = 5\n",
    "    max_len = 512\n",
    "    dropout = 0.2\n",
    "    target_size=4\n",
    "    n_accumulate=1\n",
    "    print_freq = 50\n",
    "    min_lr=1e-6\n",
    "    scheduler = 'cosine'\n",
    "    batch_size = 16\n",
    "    num_workers = 2\n",
    "    lr = 3e-5\n",
    "    weigth_decay = 0.01\n",
    "    epochs = 10\n",
    "    n_fold = 5\n",
    "    trn_fold = [0, 1, 2, 3, 4]\n",
    "    train = True \n",
    "    num_warmup_steps = 0\n",
    "    num_cycles=0.5\n",
    "    debug = False\n",
    "    debug_ver2 = False\n",
    "    gradient_checkpointing = True\n",
    "    freezing = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "136df47d-83a7-404d-b289-7784d4a9ad7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#　データセットを作成するクラスを定義します。\n",
    "class PaperDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        title = self.data.iloc[index]['title']\n",
    "        abstract = self.data.iloc[index]['abstract']\n",
    "        text = title + ' ' + abstract\n",
    "        labels = self.data.iloc[index]['y']\n",
    "        \n",
    "        # inputsの大きさをmodelが扱えるmax_length(512)に指定\n",
    "        inputs = self.tokenizer(text, max_length=512, truncation=True)\n",
    "       \n",
    "        # print(len(inputs['input_ids']))\n",
    "        \n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'],\n",
    "            'attention_mask': inputs['attention_mask'],\n",
    "            'targets': torch.tensor(labels, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc2f254d-1eec-4913-bd73-4269fedc68c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# datasetで定義されたバッチ形式の__getitem__を処理して、オリジナルのtorch.Tensorにする関数\n",
    "def collate_fn(batch):\n",
    "    input_ids = pad_sequence([torch.tensor(item[\"input_ids\"]) for item in batch], batch_first=True)\n",
    "    attention_mask = pad_sequence([torch.tensor(item[\"attention_mask\"]) for item in batch], batch_first=True)\n",
    "    # labels_0 = ([item['targets'][0] for item in batch])\n",
    "    # labels_1 = ([item['targets'][1] for item in batch])\n",
    "    # labels = torch.tensor([labels_0, labels_1])\n",
    "    labels = torch.tensor([item['targets'] for item in batch])\n",
    "\n",
    "\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, 'targets': labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac437f49-a646-4211-aad9-59dd1ed16e3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 各種データの読込\n",
    "train = pd.read_csv(train_data_path) # 訓練データの読込\n",
    "test_df = pd.read_csv(test_data_path) # テストデータの読込\n",
    "test_df['y'] = 0 # テストデータのＹの値を初期化\n",
    "submit_df = pd.read_csv(submit_data_path) # 提出用散布リファイルの読込"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee28d218-c656-4d45-b5a5-55156b1d824e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=CFG.n_splits,shuffle=True,random_state=CFG.seed)\n",
    "for fold, ( _, val_) in enumerate(skf.split(train, train.y)):\n",
    "    train.loc[val_ , \"kfold\"] = int(fold)\n",
    "    \n",
    "train[\"kfold\"] = train[\"kfold\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd04d182-a044-4269-b47e-9ca366ea886c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9494cd1cc11946268dba8aec05046ea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec116de8845f48a49603caad73227a08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7171cc7f5de040f8ab5bea6183e6c4fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ef4b56a13744a339f4baa31142af20c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.weight', 'classifier.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig\n",
    "\n",
    "#使用する学習済みモデル\n",
    "\n",
    "#model_checkpoint = \"distilbert-base-uncased\"\n",
    "model_checkpoint = \"microsoft/deberta-v3-base\"\n",
    "config = AutoConfig.from_pretrained(model_checkpoint)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, config = config)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d32a5ead-9823-40a1-8e9d-f0f73d1c5456",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DebertaV2ForSequenceClassification(\n",
       "  (deberta): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): StableDropout()\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (pooler): ContextPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): StableDropout()\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): StableDropout()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f42e380b-4bc4-4a36-97f0-2994c0948a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in dataloader:\n",
    "        \n",
    "        #分類問題なので、目的変数の型をfloatからintに変更\n",
    "        batch['targets'] = batch['targets'].type(torch.LongTensor)\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        #targets = batch['targets'].view(-1, 1).to(device)\n",
    "\n",
    "        # debaerta用\n",
    "        targets = batch['targets'].to(device)\n",
    "        train_correct = 0\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask, labels=targets)\n",
    "\n",
    "        # 損失の値\n",
    "        loss = outputs.loss\n",
    "        # それぞれのラベルの確率\n",
    "        logits = outputs.logits\n",
    "        # 確率の大きい方を取る\n",
    "        _pred= logits.argmax(-1)\n",
    "        pred = _pred.argmax(-1)\n",
    "\n",
    "        # ラベルとpredが一致しているものの数だけtrain_correctを加算\n",
    "        train_correct += pred.eq(targets).sum() \n",
    "\n",
    "        # if labels is not None:\n",
    "        #     loss = nn.functional.binary_cross_entropy_with_logits(logits, labels) * labels.shape[1]\n",
    "        # train_total += len(pred)\n",
    "        \n",
    "        #train_acc = train_correct/8 #正解率\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    return train_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2dbe5c3-6f43-4fbf-b02e-7fa5b309a2c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch['targets'] = batch['targets'].type(torch.LongTensor)\n",
    "\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            #targets = batch['targets'].view(-1, 1).to(device)\n",
    "\n",
    "            # debaerta用\n",
    "            targets = batch['targets'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask, labels=targets)\n",
    "\n",
    "            loss = outputs.loss \n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    return val_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20b618ef-9ae1-4308-8536-739970284be2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(model, dataloader):\n",
    "    outputs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            # モデルによる予測\n",
    "            output = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = output.logits\n",
    "            # 確率の大きい方を取る\n",
    "            _pred= logits.argmax(-1)\n",
    "            #pred = _pred.argmax(-1)\n",
    "            outputs.extend(_pred.tolist())\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a587813-1b4b-46d2-bfb1-ea158e1d7924",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #１バッチ(8データ)取り出す\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "for i in train_loader:\n",
    "    batch = i\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81905762-c41e-4245-a836-afa8283fecb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = PaperDataset(train, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f8d98b3-af59-4798-865e-e09e459b2d46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_ids = batch['input_ids'].to(device)\n",
    "attention_mask = batch['attention_mask'].to(device)\n",
    "targets = batch['targets'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c902bb2-4cf4-4bab-9a64-e3948390aade",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1a861938-c61b-4d4d-af7a-f15a1814ac50",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    " F.softmax(torch.tensor(outputs.logits)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c9be82f6-5212-4e13-a988-ee976a8c9d93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4973, 0.5027],\n",
       "        [0.4978, 0.5022],\n",
       "        [0.4979, 0.5021],\n",
       "        [0.4961, 0.5039],\n",
       "        [0.4974, 0.5026],\n",
       "        [0.4980, 0.5020],\n",
       "        [0.5000, 0.5000],\n",
       "        [0.4977, 0.5023]], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(outputs.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bb78819b-af35-4b9b-8fe5-d63c1e3f545e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02172697, -0.01087348],\n",
       "       [-0.02263962, -0.01391098],\n",
       "       [-0.01471355, -0.0063681 ],\n",
       "       [-0.0197665 , -0.00428472],\n",
       "       [-0.01404355, -0.00348078],\n",
       "       [-0.01907133, -0.01088125],\n",
       "       [-0.01841294, -0.01827809],\n",
       "       [-0.02060701, -0.01136107]], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits.to('cpu').detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "64c61f94-022c-46c1-adc3-741479adb095",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>abstract</th>\n",
       "      <th>keywords</th>\n",
       "      <th>y</th>\n",
       "      <th>kfold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4969</th>\n",
       "      <td>4970</td>\n",
       "      <td>Convolutional Conditional Neural Processes</td>\n",
       "      <td>2020</td>\n",
       "      <td>We introduce the Convolutional Conditional Neu...</td>\n",
       "      <td>Neural Processes, Deep Sets, Translation Equiv...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4970</th>\n",
       "      <td>4971</td>\n",
       "      <td>Gradient Descent Maximizes the Margin of Homog...</td>\n",
       "      <td>2020</td>\n",
       "      <td>In this paper, we study the implicit regulariz...</td>\n",
       "      <td>margin, homogeneous, gradient descent</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4971</th>\n",
       "      <td>4972</td>\n",
       "      <td>Adversarial Training and Provable Defenses: Br...</td>\n",
       "      <td>2020</td>\n",
       "      <td>We present COLT, a new method to train neural ...</td>\n",
       "      <td>adversarial examples, adversarial training, pr...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4972</th>\n",
       "      <td>4973</td>\n",
       "      <td>Differentiable Reasoning over a Virtual Knowle...</td>\n",
       "      <td>2020</td>\n",
       "      <td>We consider the task of answering complex mult...</td>\n",
       "      <td>Question Answering, Multi-Hop QA, Deep Learnin...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4973</th>\n",
       "      <td>4974</td>\n",
       "      <td>Federated Learning with Matched Averaging</td>\n",
       "      <td>2020</td>\n",
       "      <td>Federated learning allows edge devices to coll...</td>\n",
       "      <td>federated learning</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              title  year   \n",
       "4969  4970         Convolutional Conditional Neural Processes  2020  \\\n",
       "4970  4971  Gradient Descent Maximizes the Margin of Homog...  2020   \n",
       "4971  4972  Adversarial Training and Provable Defenses: Br...  2020   \n",
       "4972  4973  Differentiable Reasoning over a Virtual Knowle...  2020   \n",
       "4973  4974          Federated Learning with Matched Averaging  2020   \n",
       "\n",
       "                                               abstract   \n",
       "4969  We introduce the Convolutional Conditional Neu...  \\\n",
       "4970  In this paper, we study the implicit regulariz...   \n",
       "4971  We present COLT, a new method to train neural ...   \n",
       "4972  We consider the task of answering complex mult...   \n",
       "4973  Federated learning allows edge devices to coll...   \n",
       "\n",
       "                                               keywords  y  kfold  \n",
       "4969  Neural Processes, Deep Sets, Translation Equiv...  1      4  \n",
       "4970              margin, homogeneous, gradient descent  1      3  \n",
       "4971  adversarial examples, adversarial training, pr...  1      3  \n",
       "4972  Question Answering, Multi-Hop QA, Deep Learnin...  1      2  \n",
       "4973                                 federated learning  1      1  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02f2ef1-f9a0-4f4c-aad1-10d5c2a9ac09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
