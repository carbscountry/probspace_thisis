{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold,StratifiedGroupKFold,GroupKFold,train_test_split\n",
    "from sklearn.metrics import log_loss,f1_score,accuracy_score\n",
    "\n",
    "from transformers import AutoModel, AutoConfig, AutoTokenizer, AdamW, DataCollatorWithPadding\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class CFG:\n",
    "    wandb = False\n",
    "    apex = True\n",
    "    model = 'microsoft/deberta-v3-large'\n",
    "    seed = 42\n",
    "    n_splits = 5\n",
    "    max_len = 512\n",
    "    dropout = 0.2\n",
    "    target_size=4\n",
    "    n_accumulate=1\n",
    "    print_freq = 50\n",
    "    min_lr=1e-6\n",
    "    scheduler = 'cosine'\n",
    "    batch_size = 16\n",
    "    num_workers = 2\n",
    "    lr = 3e-5\n",
    "    weigth_decay = 0.01\n",
    "    epochs = 10\n",
    "    n_fold = 5\n",
    "    trn_fold = [0, 1, 2, 3, 4]\n",
    "    train = True \n",
    "    num_warmup_steps = 0\n",
    "    num_cycles=0.5\n",
    "    debug = False\n",
    "    debug_ver2 = False\n",
    "    gradient_checkpointing = True\n",
    "    freezing = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "INPUT_DIR = '/workspace/data/'\n",
    "OUTPUT_DIR = '/workspace/data/output/'\n",
    "OUTPUT_SUB_DIR = os.path.join(OUTPUT_DIR,'Submission')\n",
    "OUTPUT_MODEL_DIR = os.path.join('/workspace/model',CFG.model.replace('/','-'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loss Func\n",
    "def criterion(outputs, labels):\n",
    "    return nn.CrossEntropyLoss()(outputs, labels)\n",
    "\n",
    "def softmax(z):\n",
    "    assert len(z.shape) == 2\n",
    "    s = np.max(z, axis=1)\n",
    "    s = s[:, np.newaxis] # necessary step to do broadcasting\n",
    "    e_x = np.exp(z - s)\n",
    "    div = np.sum(e_x, axis=1)\n",
    "    div = div[:, np.newaxis] # dito\n",
    "    return e_x / div\n",
    "\"\"\"\n",
    "def get_score(y_true, y_pred):\n",
    "    y_pred = softmax(y_pred)\n",
    "    score = log_loss(y_true, y_pred)\n",
    "    return round(score, 5)\n",
    "\"\"\"\n",
    "def get_score(outputs, labels):\n",
    "    outputs = F.softmax(torch.tensor(outputs)).numpy()\n",
    "    return accuracy_score(np.argmax(outputs,axis=1),labels)\n",
    "\n",
    "def get_logger(filename=OUTPUT_DIR+'train'):\n",
    "    from logging import getLogger, INFO, FileHandler, Formatter, StreamHandler\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = get_logger()\n",
    "\n",
    "def seed_everything(seed=CFG.seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=42)\n",
    "\n",
    "def prepare_input(cfg, text, text_2=None):\n",
    "    inputs = cfg.tokenizer(text, text_2,\n",
    "                           padding=\"max_length\",\n",
    "                           add_special_tokens=True,\n",
    "                           max_length=cfg.max_len,\n",
    "                           truncation=True)\n",
    "\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "        \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def freeze(module):\n",
    "    \"\"\"\n",
    "    Freezes module's parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    for parameter in module.parameters():\n",
    "        parameter.requires_grad = False\n",
    "        \n",
    "def get_freezed_parameters(module):\n",
    "    \"\"\"\n",
    "    Returns names of freezed parameters of the given module.\n",
    "    \"\"\"\n",
    "    \n",
    "    freezed_parameters = []\n",
    "    for name, parameter in module.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            freezed_parameters.append(name)\n",
    "            \n",
    "    return freezed_parameters\n",
    "\n",
    "def set_embedding_parameters_bits(embeddings_path, optim_bits=32):\n",
    "    \"\"\"\n",
    "    https://github.com/huggingface/transformers/issues/14819#issuecomment-1003427930\n",
    "    \"\"\"\n",
    "    \n",
    "    embedding_types = (\"word\", \"position\", \"token_type\")\n",
    "    for embedding_type in embedding_types:\n",
    "        attr_name = f\"{embedding_type}_embeddings\"\n",
    "        \n",
    "        if hasattr(embeddings_path, attr_name): \n",
    "            bnb.optim.GlobalOptimManager.get_instance().register_module_override(\n",
    "                getattr(embeddings_path, attr_name), 'weight', {'optim_bits': optim_bits}\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>abstract</th>\n",
       "      <th>keywords</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Hierarchical Adversarially Learned Inference</td>\n",
       "      <td>2018</td>\n",
       "      <td>We propose a novel hierarchical generative mod...</td>\n",
       "      <td>generative, hierarchical, unsupervised, semisu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Learning to Compute Word Embeddings On the Fly</td>\n",
       "      <td>2018</td>\n",
       "      <td>Words in natural language follow a Zipfian dis...</td>\n",
       "      <td>NLU, word embeddings, representation learning</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Graph2Seq: Scalable Learning Dynamics for Graphs</td>\n",
       "      <td>2018</td>\n",
       "      <td>Neural networks are increasingly used as a gen...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Generating Differentially Private Datasets Usi...</td>\n",
       "      <td>2018</td>\n",
       "      <td>In this paper, we present a technique for gene...</td>\n",
       "      <td>generative adversarial networks, differential ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Representing dynamically: An active process fo...</td>\n",
       "      <td>2018</td>\n",
       "      <td>We propose an unsupervised method for building...</td>\n",
       "      <td>Generative Models, Latent representations, Pre...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title  year   \n",
       "0   1       Hierarchical Adversarially Learned Inference  2018  \\\n",
       "1   2     Learning to Compute Word Embeddings On the Fly  2018   \n",
       "2   3   Graph2Seq: Scalable Learning Dynamics for Graphs  2018   \n",
       "3   4  Generating Differentially Private Datasets Usi...  2018   \n",
       "4   5  Representing dynamically: An active process fo...  2018   \n",
       "\n",
       "                                            abstract   \n",
       "0  We propose a novel hierarchical generative mod...  \\\n",
       "1  Words in natural language follow a Zipfian dis...   \n",
       "2  Neural networks are increasingly used as a gen...   \n",
       "3  In this paper, we present a technique for gene...   \n",
       "4  We propose an unsupervised method for building...   \n",
       "\n",
       "                                            keywords  y  \n",
       "0  generative, hierarchical, unsupervised, semisu...  0  \n",
       "1      NLU, word embeddings, representation learning  0  \n",
       "2                                                NaN  0  \n",
       "3  generative adversarial networks, differential ...  0  \n",
       "4  Generative Models, Latent representations, Pre...  0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4974, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>abstract</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>StyleAlign: Analysis and Applications of Align...</td>\n",
       "      <td>2022</td>\n",
       "      <td>In this paper, we perform an in-depth study of...</td>\n",
       "      <td>StyleGAN, transfer learning, fine tuning, mode...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Embedding a random graph via GNN: mean-field i...</td>\n",
       "      <td>2021</td>\n",
       "      <td>We develop a theory for embedding a random gra...</td>\n",
       "      <td>Graph neural network, graph embedding, multi-r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>BBRefinement: an universal scheme to improve p...</td>\n",
       "      <td>2021</td>\n",
       "      <td>We present a conceptually simple yet powerful ...</td>\n",
       "      <td>object detection, deep neural networks, refine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Assessing Deep Reinforcement Learning Policies...</td>\n",
       "      <td>2022</td>\n",
       "      <td>Deep reinforcement learning algorithms have re...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Few-shot Learning with Big Prototypes</td>\n",
       "      <td>2022</td>\n",
       "      <td>Using dense vectors, i.e., prototypes, to repr...</td>\n",
       "      <td>Prototype, Few-shot Learning, Meta-learning</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title  year   \n",
       "0   1  StyleAlign: Analysis and Applications of Align...  2022  \\\n",
       "1   2  Embedding a random graph via GNN: mean-field i...  2021   \n",
       "2   3  BBRefinement: an universal scheme to improve p...  2021   \n",
       "3   4  Assessing Deep Reinforcement Learning Policies...  2022   \n",
       "4   5              Few-shot Learning with Big Prototypes  2022   \n",
       "\n",
       "                                            abstract   \n",
       "0  In this paper, we perform an in-depth study of...  \\\n",
       "1  We develop a theory for embedding a random gra...   \n",
       "2  We present a conceptually simple yet powerful ...   \n",
       "3  Deep reinforcement learning algorithms have re...   \n",
       "4  Using dense vectors, i.e., prototypes, to repr...   \n",
       "\n",
       "                                            keywords  \n",
       "0  StyleGAN, transfer learning, fine tuning, mode...  \n",
       "1  Graph neural network, graph embedding, multi-r...  \n",
       "2  object detection, deep neural networks, refine...  \n",
       "3                                                NaN  \n",
       "4        Prototype, Few-shot Learning, Meta-learning  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6393, 5)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(os.path.join(INPUT_DIR, 'train_data.csv'))\n",
    "test = pd.read_csv(os.path.join(INPUT_DIR, 'test_data.csv'))\n",
    "submission_df = pd.read_csv(os.path.join(INPUT_DIR, 'submission.csv'))\n",
    "\n",
    "display(train.head())\n",
    "print(train.shape)\n",
    "display(test.head())\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=CFG.n_splits,shuffle=True,random_state=CFG.seed)\n",
    "for fold, ( _, val_) in enumerate(skf.split(train, train.y)):\n",
    "    train.loc[val_ , \"kfold\"] = int(fold)\n",
    "    \n",
    "train[\"kfold\"] = train[\"kfold\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model)\n",
    "tokenizer.save_pretrained(os.path.join(OUTPUT_MODEL_DIR,CFG.model.replace('/','-'))+'tokenizer/')\n",
    "CFG.tokenizer = tokenizer\n",
    "SEP = tokenizer.sep_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length):\n",
    "        self.df = df\n",
    "        self.max_len = CFG.max_len\n",
    "        self.text = df['abstract'].values\n",
    "        self.tokenizer = CFG.tokenizer\n",
    "        self.targets = df['y'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.text[index]\n",
    "        inputs = tokenizer.encode_plus(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length = self.max_len\n",
    "        )\n",
    "        samples = {\n",
    "            'input_ids': inputs['input_ids'],\n",
    "            'attention_mask': inputs['attention_mask'],\n",
    "            'target': self.targets[index]\n",
    "        }\n",
    "\n",
    "        if 'token_type_ids' in inputs:\n",
    "            samples['token_type_ids'] = inputs['token_type_ids']\n",
    "            \n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dynamic Padding (Collate)\n",
    "#collate_fn = DataCollatorWithPadding(tokenizer=CFG.tokenizer)\n",
    "class Collate:\n",
    "    def __init__(self, tokenizer, isTrain=True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.isTrain = isTrain\n",
    "        # self.args = args\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        output = dict()\n",
    "        output[\"input_ids\"] = [sample[\"input_ids\"] for sample in batch]\n",
    "        output[\"attention_mask\"] = [sample[\"attention_mask\"] for sample in batch]\n",
    "        if self.isTrain:\n",
    "            output[\"target\"] = [sample[\"target\"] for sample in batch]\n",
    "\n",
    "        # calculate max token length of this batch\n",
    "        batch_max = max([len(ids) for ids in output[\"input_ids\"]])\n",
    "\n",
    "        # add padding\n",
    "        if self.tokenizer.padding_side == \"right\":\n",
    "            output[\"input_ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"input_ids\"]]\n",
    "            output[\"attention_mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"attention_mask\"]]\n",
    "        else:\n",
    "            output[\"input_ids\"] = [(batch_max - len(s)) * [self.tokenizer.pad_token_id] + s for s in output[\"input_ids\"]]\n",
    "            output[\"attention_mask\"] = [(batch_max - len(s)) * [0] + s for s in output[\"attention_mask\"]]\n",
    "\n",
    "        # convert to tensors\n",
    "        output[\"input_ids\"] = torch.tensor(output[\"input_ids\"], dtype=torch.long)\n",
    "        output[\"attention_mask\"] = torch.tensor(output[\"attention_mask\"], dtype=torch.long)\n",
    "        if self.isTrain:\n",
    "            output[\"target\"] = torch.tensor(output[\"target\"], dtype=torch.long)\n",
    "\n",
    "        return output\n",
    "    \n",
    "collate_fn = Collate(CFG.tokenizer, isTrain=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "        \n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9) #\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Model\n",
    "# ====================================================\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(CustomModel, self).__init__()\n",
    "        # Header (fast or normal)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Gradient_checkpointing\n",
    "        if CFG.gradient_checkpointing:\n",
    "            (self.model).gradient_checkpointing_enable()\n",
    "        \n",
    "        # Freezing\n",
    "        if CFG.freezing:\n",
    "            # freezing embeddings and first 2 layers of encoder\n",
    "            freeze((self.model).embeddings)\n",
    "            freeze((self.model).encoder.layer[:2])\n",
    "            CFG.after_freezed_parameters = filter(lambda parameter: parameter.requires_grad, (self.model).parameters())\n",
    "        \n",
    "        self.config = AutoConfig.from_pretrained(model_name)\n",
    "        self.drop = nn.Dropout(p=CFG.dropout)\n",
    "        self.pooler = MeanPooling()\n",
    "        self.fc = nn.Linear(self.config.hidden_size, CFG.target_size)\n",
    "        \n",
    "    def forward(self, ids, mask):        \n",
    "        out = self.model(input_ids=ids, \n",
    "                         attention_mask=mask,\n",
    "                         output_hidden_states=False)\n",
    "        out = self.pooler(out.last_hidden_state, mask)\n",
    "        out = self.drop(out)\n",
    "        outputs = self.fc(out)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def asMinutes(s):\n",
    "    m = math.floor(s/60)\n",
    "    s -= m * 60\n",
    "    return \"%dm %ds\" % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "    if cfg.scheduler == 'linear':\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n",
    "        )\n",
    "    elif cfg.scheduler == 'cosine':\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n",
    "        )\n",
    "    return scheduler\n",
    "\n",
    "def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n",
    "    model.train()\n",
    "\n",
    "    dataset_size = 0\n",
    "    running_loss = 0\n",
    "\n",
    "    start = end = time.time()\n",
    "\n",
    "    for step, data in enumerate(dataloader):\n",
    "        ids = data['input_ids'].to(device, dtype=torch.long)\n",
    "        mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "        targets = data['target'].to(device, dtype=torch.long)\n",
    "\n",
    "        batch_size = ids.size(0)\n",
    "        \n",
    "        outputs = model(ids, mask)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        #accumulate\n",
    "        loss = loss / CFG.n_accumulate \n",
    "        loss.backward()\n",
    "        if (step +1) % CFG.n_accumulate == 0:\n",
    "            optimizer.step()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        dataset_size += batch_size\n",
    "\n",
    "        epoch_loss = running_loss / dataset_size\n",
    "\n",
    "        end = time.time()\n",
    "        \n",
    "        if step % CFG.print_freq == 0 or step == (len(dataloader)-1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  .format(epoch+1, step, len(dataloader), \n",
    "                          remain=timeSince(start, float(step+1)/len(dataloader))))\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    return epoch_loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def valid_one_epoch(model, dataloader, device, epoch):\n",
    "    model.eval()\n",
    "\n",
    "    dataset_size = 0\n",
    "    running_loss = 0\n",
    "\n",
    "    start = end = time.time()\n",
    "    pred = []\n",
    "\n",
    "    for step, data in enumerate(dataloader):\n",
    "        ids = data['input_ids'].to(device, dtype=torch.long)\n",
    "        mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "        targets = data['target'].to(device, dtype=torch.long)\n",
    "\n",
    "        batch_size = ids.size(0)\n",
    "        outputs = model(ids, mask)\n",
    "        loss = criterion(outputs, targets)\n",
    "        pred.append(outputs.to('cpu').numpy())\n",
    "\n",
    "        running_loss += (loss.item()* batch_size)\n",
    "        dataset_size += batch_size\n",
    "\n",
    "        epoch_loss = running_loss / dataset_size\n",
    "\n",
    "        end = time.time()\n",
    "\n",
    "        if step % CFG.print_freq == 0 or step == (len(dataloader)-1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  .format(step, len(dataloader),\n",
    "                          remain=timeSince(start, float(step+1)/len(dataloader))))\n",
    "            \n",
    "    pred = np.concatenate(pred)\n",
    "            \n",
    "    return epoch_loss, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_loop(fold):\n",
    "    #wandb.watch(model, log_freq=100)\n",
    "\n",
    "    LOGGER.info(f'-------------fold:{fold} training-------------')\n",
    "\n",
    "    train_data = train[train.kfold != fold].reset_index(drop=True)\n",
    "    valid_data = train[train.kfold == fold].reset_index(drop=True)\n",
    "    valid_labels = valid_data.y.values\n",
    "\n",
    "    trainDataset = Dataset(train_data, CFG.tokenizer, CFG.max_len)\n",
    "    validDataset = Dataset(valid_data, CFG.tokenizer, CFG.max_len)\n",
    "\n",
    "    train_loader = DataLoader(trainDataset,\n",
    "                              batch_size = CFG.batch_size,\n",
    "                              shuffle=True,\n",
    "                              collate_fn = collate_fn,\n",
    "                              num_workers = CFG.num_workers,\n",
    "                              pin_memory = True,\n",
    "                              drop_last=True)\n",
    "    \n",
    "    valid_loader = DataLoader(validDataset,\n",
    "                              batch_size = CFG.batch_size*2,\n",
    "                              shuffle=False,\n",
    "                              collate_fn = collate_fn,\n",
    "                              num_workers = CFG.num_workers,\n",
    "                              pin_memory = True,\n",
    "                              drop_last=False)\n",
    "    \n",
    "    model = CustomModel(CFG.model)\n",
    "    torch.save(model.config,OUTPUT_MODEL_DIR+'config.pth')\n",
    "    model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=CFG.lr, weight_decay=CFG.weigth_decay)\n",
    "    num_train_steps = int(len(train_data) / CFG.batch_size * CFG.epochs)\n",
    "    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n",
    "\n",
    "    # loop\n",
    "    best_score = 0\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_epoch_loss = train_one_epoch(model, optimizer, scheduler, train_loader, device, epoch)\n",
    "        valid_epoch_loss, pred = valid_one_epoch(model, valid_loader, device, epoch)\n",
    "\n",
    "        score = get_score(pred, valid_labels)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {train_epoch_loss:.4f}  avg_val_loss: {valid_epoch_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}')\n",
    "        if CFG.wandb:\n",
    "            wandb.log({f\"[fold{fold}] epoch\": epoch+1, \n",
    "                       f\"[fold{fold}] avg_train_loss\": train_epoch_loss, \n",
    "                       f\"[fold{fold}] avg_val_loss\": valid_epoch_loss,\n",
    "                       f\"[fold{fold}] score\": score})\n",
    "            \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save({'model': model.state_dict(),\n",
    "                        'predictions': pred},\n",
    "                        OUTPUT_MODEL_DIR+f\"_fold{fold}_best.pth\")\n",
    "            \n",
    "    predictions = torch.load(OUTPUT_MODEL_DIR+f\"_fold{fold}_best.pth\", \n",
    "                             map_location=torch.device('cpu'))['predictions']\n",
    "    valid_data['0'] = predictions[:, 0]\n",
    "    valid_data['1'] = predictions[:, 1]\n",
    "    \n",
    "    \n",
    "    temp = valid_data[['0','1']].values.tolist()\n",
    "    print(get_score(temp, valid_data['y'].values))\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "CFG.epochs = 10\n",
    "CFG.max_len = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-------------fold:0 training-------------\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/248] Elapsed 0m 1s (remain 7m 33s) \n",
      "Epoch: [1][50/248] Elapsed 0m 48s (remain 3m 7s) \n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    def get_result(oof_df):\n",
    "        labels = oof_df['y'].values\n",
    "        preds = oof_df[['0','1']].values.tolist()\n",
    "        score = get_score(preds, labels)\n",
    "        LOGGER.info(f'Score: {score:<.4f}')\n",
    "    \n",
    "    if CFG.train:\n",
    "        oof_df = pd.DataFrame()\n",
    "        # for fold in range(CFG.n_fold):\n",
    "        #     if fold in CFG.trn_fold:\n",
    "        #         _oof_df = train_loop(fold)\n",
    "        #         oof_df = pd.concat([oof_df, _oof_df])\n",
    "        #         LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "        #         get_result(_oof_df)\n",
    "        _oof_df = train_loop(0)\n",
    "        oof_df = pd.concat([oof_df, _oof_df])\n",
    "        LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "        get_result(_oof_df)\n",
    "        oof_df = oof_df.reset_index(drop=True)\n",
    "        LOGGER.info(f\"========== CV ==========\")\n",
    "        get_result(oof_df)\n",
    "        oof_df.to_pickle(OUTPUT_MODEL_DIR+'oof_df.pkl')\n",
    "        oof_df.to_csv(OUTPUT_MODEL_DIR+f'oof_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
