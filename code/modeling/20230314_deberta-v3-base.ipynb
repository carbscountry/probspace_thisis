{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import BertModel, BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# 各種パスの設定\n",
    "######################################################################\n",
    "base_path = '/workspace' # ベースとなるパスを指定してください。#######\n",
    "######################################################################\n",
    "os.makedirs(os.path.join(base_path,'model'), exist_ok=True)  # 学習済みモデルの保存するディレクトリを作成\n",
    "os.makedirs(os.path.join(base_path,'output'), exist_ok=True)  # 提出用ファイルを出力するディレクトリを作成\n",
    "train_data_path = os.path.join(base_path,'data/StratifiedKFold_train_data.csv') # 訓練データのパスを指定\n",
    "test_data_path = os.path.join(base_path,'data/test_data.csv') # テストデータのパスを指定\n",
    "submit_data_path = os.path.join(base_path,'data/submission.csv') # 提出用サンプルfileのパスを指定\n",
    "model_file_path = os.path.join(base_path,'model') # 学習済みモデルのパスを指定\n",
    "output_file_path = os.path.join(base_path,'output/20230303_submission.csv') # 提出用ファイルのパスを指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#　データセットを作成するクラスを定義します。\n",
    "class PaperDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        title = self.data.iloc[index]['title']\n",
    "        abstract = self.data.iloc[index]['abstract']\n",
    "        text = title + ' ' + abstract\n",
    "        labels = self.data.iloc[index]['y']\n",
    "        \n",
    "        # inputsの大きさをmodelが扱えるmax_length(512)に指定\n",
    "        inputs = self.tokenizer(text, max_length=512, truncation=True)\n",
    "       \n",
    "        # print(len(inputs['input_ids']))\n",
    "        \n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'],\n",
    "            'attention_mask': inputs['attention_mask'],\n",
    "            'targets': torch.tensor(labels, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# datasetで定義されたバッチ形式の__getitem__を処理して、オリジナルのtorch.Tensorにする関数\n",
    "def collate_fn(batch):\n",
    "    input_ids = pad_sequence([torch.tensor(item[\"input_ids\"]) for item in batch], batch_first=True)\n",
    "    attention_mask = pad_sequence([torch.tensor(item[\"attention_mask\"]) for item in batch], batch_first=True)\n",
    "    # labels_0 = ([item['targets'][0] for item in batch])\n",
    "    # labels_1 = ([item['targets'][1] for item in batch])\n",
    "    # labels = torch.tensor([labels_0, labels_1])\n",
    "    labels = torch.tensor([item['targets'] for item in batch])\n",
    "\n",
    "\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, 'targets': labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 各種データの読込\n",
    "df = pd.read_csv(train_data_path) # 訓練データの読込\n",
    "test_df = pd.read_csv(test_data_path) # テストデータの読込\n",
    "test_df['y'] = 0 # テストデータのＹの値を初期化\n",
    "submit_df = pd.read_csv(submit_data_path) # 提出用散布リファイルの読込"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig\n",
    "\n",
    "#使用する学習済みモデル\n",
    "\n",
    "#model_checkpoint = \"distilbert-base-uncased\"\n",
    "model_checkpoint = \"microsoft/deberta-v3-base\"\n",
    "config = AutoConfig.from_pretrained(model_checkpoint)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, config = config)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in dataloader:\n",
    "        \n",
    "        #分類問題なので、目的変数の型をfloatからintに変更\n",
    "        batch['targets'] = batch['targets'].type(torch.LongTensor)\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        #targets = batch['targets'].view(-1, 1).to(device)\n",
    "\n",
    "        # debaerta用\n",
    "        targets = batch['targets'].to(device)\n",
    "        train_correct = 0\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask, labels=targets)\n",
    "\n",
    "        # 損失の値\n",
    "        loss = outputs.loss\n",
    "        # それぞれのラベルの確率\n",
    "        logits = outputs.logits\n",
    "        # 確率の大きい方を取る\n",
    "        _pred= logits.argmax(-1)\n",
    "        pred = _pred.argmax(-1)\n",
    "\n",
    "        # ラベルとpredが一致しているものの数だけtrain_correctを加算\n",
    "        train_correct += pred.eq(targets).sum() \n",
    "\n",
    "        # if labels is not None:\n",
    "        #     loss = nn.functional.binary_cross_entropy_with_logits(logits, labels) * labels.shape[1]\n",
    "        # train_total += len(pred)\n",
    "        \n",
    "        #train_acc = train_correct/8 #正解率\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    return train_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch['targets'] = batch['targets'].type(torch.LongTensor)\n",
    "\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            #targets = batch['targets'].view(-1, 1).to(device)\n",
    "\n",
    "            # debaerta用\n",
    "            targets = batch['targets'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask, labels=targets)\n",
    "\n",
    "            loss = outputs.loss \n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    return val_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(model, dataloader):\n",
    "    outputs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            # モデルによる予測\n",
    "            output = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = output.logits\n",
    "            # 確率の大きい方を取る\n",
    "            _pred= logits.argmax(-1)\n",
    "            #pred = _pred.argmax(-1)\n",
    "            outputs.extend(_pred.tolist())\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"y_pred\"] = 0\n",
    "for fold in df[\"folds\"].unique():\n",
    "    train_df = df[df[\"folds\"] != fold]\n",
    "    val_df = df[df[\"folds\"] == fold]\n",
    "    train_df = train_df.reset_index()\n",
    "    val_df = val_df.reset_index()\n",
    "    train_dataset = PaperDataset(train_df, tokenizer)\n",
    "    val_dataset = PaperDataset(val_df, tokenizer)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, collate_fn=collate_fn)\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in tqdm(range(4)):\n",
    "        print(epoch)\n",
    "        train_loss = train(model, train_loader, optimizer)\n",
    "        val_loss = validate(model, val_loader, optimizer)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), os.path.join(model_file_path,f\"deberta-v3-base_{fold}_model.pth\")) # ベストなモデルを保存する\n",
    "        print(f\"{fold}_fold\")\n",
    "        #予測の実行\n",
    "        outputs = predict(model, val_loader)\n",
    "        df.loc[val_df[\"y_pred\"].index.to_list(),\"y_pred\"] = pd.Series(outputs)\n",
    "        acc = accuracy_score(val_df[\"y\"], df.loc[val_df[\"y_pred\"].index.to_list(),\"y_pred\"])\n",
    "        print(f\"acc:{acc}\")\n",
    "        print(f'Epoch {epoch+1} - train loss: {train_loss:.3f}, val loss: {val_loss:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"/workspace/data/output/preds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #テスト\n",
    "\n",
    "# # モデルの読み込み\n",
    "# model.load_state_dict(torch.load(model_file_path))\n",
    "\n",
    "# test_dataset = PaperDataset(test_df, tokenizer)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #予測の実行\n",
    "# outputs = predict(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit_df['y'] = pd.Series(outputs)\n",
    "# submit_df.to_csv(output_file_path,index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 検証用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #１バッチ(8データ)取り出す\n",
    "# for i in train_loader:\n",
    "#     batch = i\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch['targets'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch['targets'] = batch['targets'].type(torch.LongTensor)\n",
    "batch['targets'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids = batch['input_ids'].to(device)\n",
    "# attention_mask = batch['attention_mask'].to(device)\n",
    "# targets = batch['targets'].to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_index = (targets >= 0).nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_index.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs = model(input_ids, attention_mask, labels=targets)\n",
    "# outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs = model(input_ids, attention_mask=attention_mask)\n",
    "# logits = outputs.logits\n",
    "# #print(logits)\n",
    "# # 確率の大きい方を取る\n",
    "# _pred= logits.argmax(-1)\n",
    "\n",
    "# print(_pred)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "aa38bdac91f012d3aa63752192202c1c4a31351885bf71923469150acc0ed662"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
