{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import BertModel, BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# 各種パスの設定\n",
    "######################################################################\n",
    "base_path = '/workspace' # ベースとなるパスを指定してください。#######\n",
    "######################################################################\n",
    "os.makedirs(os.path.join(base_path,'model'), exist_ok=True)  # 学習済みモデルの保存するディレクトリを作成\n",
    "os.makedirs(os.path.join(base_path,'output'), exist_ok=True)  # 提出用ファイルを出力するディレクトリを作成\n",
    "train_data_path = os.path.join(base_path,'data/train_data.csv') # 訓練データのパスを指定\n",
    "test_data_path = os.path.join(base_path,'data/test_data.csv') # テストデータのパスを指定\n",
    "submit_data_path = os.path.join(base_path,'data/submission.csv') # 提出用サンプルfileのパスを指定\n",
    "model_file_path = os.path.join(base_path,'model') # 学習済みモデルのパスを指定\n",
    "output_file_path = os.path.join(base_path,'output/20230303_submission.csv') # 提出用ファイルのパスを指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    wandb = False\n",
    "    apex = True\n",
    "    model = 'microsoft/deberta-v3-large'\n",
    "    seed = 42\n",
    "    n_splits = 5\n",
    "    max_len = 512\n",
    "    dropout = 0.2\n",
    "    target_size=4\n",
    "    n_accumulate=1\n",
    "    print_freq = 50\n",
    "    min_lr=1e-6\n",
    "    scheduler = 'cosine'\n",
    "    batch_size = 16\n",
    "    num_workers = 2\n",
    "    lr = 3e-5\n",
    "    weigth_decay = 0.01\n",
    "    epochs = 10\n",
    "    n_fold = 5\n",
    "    trn_fold = [0, 1, 2, 3, 4]\n",
    "    train = True \n",
    "    num_warmup_steps = 0\n",
    "    num_cycles=0.5\n",
    "    debug = False\n",
    "    debug_ver2 = False\n",
    "    gradient_checkpointing = True\n",
    "    freezing = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = '/workspace/data/'\n",
    "OUTPUT_DIR = '/workspace/data/output/'\n",
    "OUTPUT_SUB_DIR = os.path.join(OUTPUT_DIR,'Submission')\n",
    "OUTPUT_MODEL_DIR = os.path.join('/workspace/model',CFG.model.replace('/','-'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#　データセットを作成するクラスを定義します。\n",
    "class PaperDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        title = self.data.iloc[index]['title']\n",
    "        abstract = self.data.iloc[index]['abstract']\n",
    "        text = title + ' ' + abstract\n",
    "        labels = self.data.iloc[index]['y']\n",
    "        \n",
    "        # inputsの大きさをmodelが扱えるmax_length(512)に指定\n",
    "        inputs = self.tokenizer(text, max_length=512, truncation=True)\n",
    "       \n",
    "        # print(len(inputs['input_ids']))\n",
    "        \n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'],\n",
    "            'attention_mask': inputs['attention_mask'],\n",
    "            'targets': torch.tensor(labels, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# datasetで定義されたバッチ形式の__getitem__を処理して、オリジナルのtorch.Tensorにする関数\n",
    "def collate_fn(batch):\n",
    "    input_ids = pad_sequence([torch.tensor(item[\"input_ids\"]) for item in batch], batch_first=True)\n",
    "    attention_mask = pad_sequence([torch.tensor(item[\"attention_mask\"]) for item in batch], batch_first=True)\n",
    "    # labels_0 = ([item['targets'][0] for item in batch])\n",
    "    # labels_1 = ([item['targets'][1] for item in batch])\n",
    "    # labels = torch.tensor([labels_0, labels_1])\n",
    "    labels = torch.tensor([item['targets'] for item in batch])\n",
    "\n",
    "\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, 'targets': labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 各種データの読込\n",
    "train_df = pd.read_csv(train_data_path) # 訓練データの読込\n",
    "test_df = pd.read_csv(test_data_path) # テストデータの読込\n",
    "test_df['y'] = 0 # テストデータのＹの値を初期化\n",
    "submit_df = pd.read_csv(submit_data_path) # 提出用散布リファイルの読込"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=CFG.n_splits,shuffle=True,random_state=CFG.seed)\n",
    "for fold, ( _, val_) in enumerate(skf.split(train_df, train_df.y)):\n",
    "    train_df.loc[val_ , \"kfold\"] = int(fold)\n",
    "    \n",
    "train_df[\"kfold\"] = train_df[\"kfold\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.weight', 'classifier.weight', 'classifier.bias', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig\n",
    "\n",
    "#使用する学習済みモデル\n",
    "\n",
    "#model_checkpoint = \"distilbert-base-uncased\"\n",
    "model_checkpoint = \"microsoft/deberta-v3-base\"\n",
    "config = AutoConfig.from_pretrained(model_checkpoint)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, config = config)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DebertaV2ForSequenceClassification(\n",
       "  (deberta): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): StableDropout()\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (pooler): ContextPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): StableDropout()\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): StableDropout()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    assert len(z.shape) == 2\n",
    "    s = np.max(z, axis=1)\n",
    "    s = s[:, np.newaxis] # necessary step to do broadcasting\n",
    "    e_x = np.exp(z - s)\n",
    "    div = np.sum(e_x, axis=1)\n",
    "    div = div[:, np.newaxis] # dito\n",
    "    return e_x / div\n",
    "\n",
    "def get_score(outputs, labels):\n",
    "    outputs = F.softmax(torch.tensor(outputs)).numpy()\n",
    "    return accuracy_score(np.argmax(outputs,axis=1),labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in dataloader:\n",
    "        \n",
    "        #分類問題なので、目的変数の型をfloatからintに変更\n",
    "        batch['targets'] = batch['targets'].type(torch.LongTensor)\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        #targets = batch['targets'].view(-1, 1).to(device)\n",
    "\n",
    "        # debaerta用\n",
    "        targets = batch['targets'].to(device)\n",
    "        train_correct = 0\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask, labels=targets)\n",
    "\n",
    "        # 損失の値\n",
    "        loss = outputs.loss\n",
    "        # それぞれのラベルの確率\n",
    "        logits = outputs.logits\n",
    "        # 確率の大きい方を取る\n",
    "        _pred= logits.argmax(-1)\n",
    "        pred = _pred.argmax(-1)\n",
    "\n",
    "        # ラベルとpredが一致しているものの数だけtrain_correctを加算\n",
    "        train_correct += pred.eq(targets).sum() \n",
    "\n",
    "        # if labels is not None:\n",
    "        #     loss = nn.functional.binary_cross_entropy_with_logits(logits, labels) * labels.shape[1]\n",
    "        # train_total += len(pred)\n",
    "        \n",
    "        #train_acc = train_correct/8 #正解率\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return train_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch['targets'] = batch['targets'].type(torch.LongTensor)\n",
    "\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            #targets = batch['targets'].view(-1, 1).to(device)\n",
    "\n",
    "            # debaerta用\n",
    "            targets = batch['targets'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask, labels=targets)\n",
    "\n",
    "            loss = outputs.loss \n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    return val_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(model, dataloader,val_df):\n",
    "    outputs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            # モデルによる予測\n",
    "            output = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = output.logits\n",
    "            # 確率の大きい方を取る\n",
    "            _pred= F.softmax(logits.to('cpu'))\n",
    "            #pred = _pred.argmax(-1)\n",
    "            outputs.append(_pred.to('cpu').detach().numpy())\n",
    "        outputs = np.concatenate(outputs)[:,0]\n",
    "        oof_df = val_df.copy()\n",
    "        oof_df['0'] = list(outputs[:,0])\n",
    "        oof_df['1'] = list(outputs[:,1])\n",
    "        torch.cuda.empty_cache()\n",
    "    return outputs,oof_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_ave = []\n",
    "oof_df = pd.DataFrame()\n",
    "for fold in train_df[\"kfold\"].unique():\n",
    "    train_df = train_df[train_df[\"kfold\"] != fold]\n",
    "    val_df = train_df[train_df[\"kfold\"] == fold]\n",
    "    train_df = train_df.reset_index()\n",
    "    val_df = val_df.reset_index()\n",
    "    train_dataset = PaperDataset(train_df, tokenizer)\n",
    "    val_dataset = PaperDataset(val_df, tokenizer)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, collate_fn=collate_fn)\n",
    "    \n",
    "    \n",
    "    for epoch in tqdm(range(1)):\n",
    "        print(epoch)\n",
    "        train_loss = train(model, train_loader, optimizer)\n",
    "        val_loss = validate(model, val_loader, optimizer)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), os.path.join(OUTPUT_MODEL_DIR,f\"{fold}_model.pth\")) # ベストなモデルを保存する\n",
    "        print(f\"{fold}_fold\")\n",
    "    \n",
    "        model.load_state_dict(torch.load(os.path.join(OUTPUT_MODEL_DIR,f\"{fold}_model.pth\"))) #モデルのロード\n",
    "        outputs,_oof_df = predict(model, val_loader,val_df)\n",
    "        oof_df = pd.concat([oof_df, _oof_df])\n",
    "        acc = get_score(outputs,val_df[\"y\"])\n",
    "        acc_ave.append(acc)\n",
    "        print(f\"acc:{acc}\")\n",
    "        print(f'Epoch {epoch+1} - train loss: {train_loss:.3f}, val loss: {val_loss:.3f}')\n",
    "oof_df.to_csv(OUTPUT_MODEL_DIR+f'oof_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0_fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [03:40<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfold\u001b[39m}\u001b[39;00m\u001b[39m_fold\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(OUTPUT_MODEL_DIR,\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfold\u001b[39m}\u001b[39;00m\u001b[39m_model.pth\u001b[39m\u001b[39m\"\u001b[39m))) \u001b[39m#モデルのロード\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m outputs,_oof_df \u001b[39m=\u001b[39m predict(model, val_loader,val_df)\n\u001b[1;32m     22\u001b[0m oof_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([oof_df, _oof_df])\n\u001b[1;32m     23\u001b[0m acc \u001b[39m=\u001b[39m get_score(outputs,val_df[\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "Cell \u001b[0;32mIn[13], line 15\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(model, dataloader, val_df)\u001b[0m\n\u001b[1;32m     13\u001b[0m     outputs\u001b[39m.\u001b[39mappend(_pred\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy())\n\u001b[1;32m     14\u001b[0m oof_df \u001b[39m=\u001b[39m val_df\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m---> 15\u001b[0m oof_df[\u001b[39m'\u001b[39m\u001b[39m0\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(outputs[:,\u001b[39m0\u001b[39;49m])\n\u001b[1;32m     16\u001b[0m oof_df[\u001b[39m'\u001b[39m\u001b[39m1\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(outputs[:,\u001b[39m1\u001b[39m])\n\u001b[1;32m     17\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "fold = 0\n",
    "val_train_df = train_df[train_df[\"kfold\"] != fold].reset_index()\n",
    "val_df = train_df[train_df[\"kfold\"] == fold].reset_index()\n",
    "train_dataset = PaperDataset(val_train_df, tokenizer)\n",
    "val_dataset = PaperDataset(val_df, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(1)):\n",
    "    print(epoch)\n",
    "    train_loss = train(model, train_loader, optimizer)\n",
    "    val_loss = validate(model, val_loader, optimizer)\n",
    "\n",
    "    # if val_loss < best_val_loss:\n",
    "    #     best_val_loss = val_loss\n",
    "    torch.save(model.state_dict(), os.path.join(OUTPUT_MODEL_DIR,f\"{fold}_model.pth\")) # ベストなモデルを保存する\n",
    "    print(f\"{fold}_fold\")\n",
    "\n",
    "    model.load_state_dict(torch.load(os.path.join(OUTPUT_MODEL_DIR,f\"{fold}_model.pth\"))) #モデルのロード\n",
    "    outputs,_oof_df = predict(model, val_loader,val_df)\n",
    "    oof_df = pd.concat([oof_df, _oof_df])\n",
    "    acc = get_score(outputs,val_df[\"y\"])\n",
    "    acc_ave.append(acc)\n",
    "    print(f\"acc:{acc}\")\n",
    "    print(f'Epoch {epoch+1} - train loss: {train_loss:.3f}, val loss: {val_loss:.3f}')\n",
    "oof_df.to_csv(OUTPUT_MODEL_DIR+f'oof_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        # モデルによる予測\n",
    "        output = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = output.logits\n",
    "        # 確率の大きい方を取る\n",
    "        _pred= F.softmax(logits.to('cpu'))\n",
    "        #pred = _pred.argmax(-1)\n",
    "        outputs.append(_pred.to('cpu').detach().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "aa38bdac91f012d3aa63752192202c1c4a31351885bf71923469150acc0ed662"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
