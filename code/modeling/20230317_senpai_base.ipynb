{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold,StratifiedGroupKFold,GroupKFold\n",
    "from sklearn.metrics import log_loss,f1_score\n",
    "\n",
    "from transformers import AutoModel, AutoConfig, AutoTokenizer, AdamW, DataCollatorWithPadding\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = '/workspace/data/'\n",
    "OUTPUT_DIR = '/workspace/data/output/'\n",
    "OUTPUT_SUB_DIR = os.path.join(OUTPUT_DIR,'Submission')\n",
    "OUTPUT_MODEL_DIR = '/workspace/model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CFG:\n",
    "    wandb = False\n",
    "    apex = True\n",
    "    model = 'microsoft/deberta-v3-large'\n",
    "    seed = 42\n",
    "    n_splits = 5\n",
    "    max_len = 512\n",
    "    dropout = 0.2\n",
    "    target_size=4\n",
    "    n_accumulate=1\n",
    "    print_freq = 50\n",
    "    min_lr=1e-6\n",
    "    scheduler = 'cosine'\n",
    "    batch_size = 16\n",
    "    num_workers = 2\n",
    "    lr = 3e-5\n",
    "    weigth_decay = 0.01\n",
    "    epochs = 10\n",
    "    n_fold = 5\n",
    "    trn_fold = [0, 1, 2, 3, 4]\n",
    "    train = True \n",
    "    num_warmup_steps = 0\n",
    "    num_cycles=0.5\n",
    "    debug = False\n",
    "    debug_ver2 = False\n",
    "    gradient_checkpointing = True\n",
    "    freezing = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Func\n",
    "def criterion(outputs, labels):\n",
    "    return nn.CrossEntropyLoss()(outputs, labels)\n",
    "\n",
    "def softmax(z):\n",
    "    assert len(z.shape) == 2\n",
    "    s = np.max(z, axis=1)\n",
    "    s = s[:, np.newaxis] # necessary step to do broadcasting\n",
    "    e_x = np.exp(z - s)\n",
    "    div = np.sum(e_x, axis=1)\n",
    "    div = div[:, np.newaxis] # dito\n",
    "    return e_x / div\n",
    "\"\"\"\n",
    "def get_score(y_true, y_pred):\n",
    "    y_pred = softmax(y_pred)\n",
    "    score = log_loss(y_true, y_pred)\n",
    "    return round(score, 5)\n",
    "\"\"\"\n",
    "def get_score(outputs, labels):\n",
    "    outputs = F.softmax(torch.tensor(outputs)).numpy()\n",
    "    return f1_score(np.argmax(outputs,axis=1),labels ,average='macro')\n",
    "\n",
    "def get_logger(filename=OUTPUT_DIR+'train'):\n",
    "    from logging import getLogger, INFO, FileHandler, Formatter, StreamHandler\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = get_logger()\n",
    "\n",
    "def seed_everything(seed=CFG.seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=42)\n",
    "\n",
    "def prepare_input(cfg, text, text_2=None):\n",
    "    inputs = cfg.tokenizer(text, text_2,\n",
    "                           padding=\"max_length\",\n",
    "                           add_special_tokens=True,\n",
    "                           max_length=cfg.max_len,\n",
    "                           truncation=True)\n",
    "\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "        \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def freeze(module):\n",
    "    \"\"\"\n",
    "    Freezes module's parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    for parameter in module.parameters():\n",
    "        parameter.requires_grad = False\n",
    "        \n",
    "def get_freezed_parameters(module):\n",
    "    \"\"\"\n",
    "    Returns names of freezed parameters of the given module.\n",
    "    \"\"\"\n",
    "    \n",
    "    freezed_parameters = []\n",
    "    for name, parameter in module.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            freezed_parameters.append(name)\n",
    "            \n",
    "    return freezed_parameters\n",
    "\n",
    "def set_embedding_parameters_bits(embeddings_path, optim_bits=32):\n",
    "    \"\"\"\n",
    "    https://github.com/huggingface/transformers/issues/14819#issuecomment-1003427930\n",
    "    \"\"\"\n",
    "    \n",
    "    embedding_types = (\"word\", \"position\", \"token_type\")\n",
    "    for embedding_type in embedding_types:\n",
    "        attr_name = f\"{embedding_type}_embeddings\"\n",
    "        \n",
    "        if hasattr(embeddings_path, attr_name): \n",
    "            bnb.optim.GlobalOptimManager.get_instance().register_module_override(\n",
    "                getattr(embeddings_path, attr_name), 'weight', {'optim_bits': optim_bits}\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>abstract</th>\n",
       "      <th>keywords</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Hierarchical Adversarially Learned Inference</td>\n",
       "      <td>2018</td>\n",
       "      <td>We propose a novel hierarchical generative mod...</td>\n",
       "      <td>generative, hierarchical, unsupervised, semisu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Learning to Compute Word Embeddings On the Fly</td>\n",
       "      <td>2018</td>\n",
       "      <td>Words in natural language follow a Zipfian dis...</td>\n",
       "      <td>NLU, word embeddings, representation learning</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Graph2Seq: Scalable Learning Dynamics for Graphs</td>\n",
       "      <td>2018</td>\n",
       "      <td>Neural networks are increasingly used as a gen...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Generating Differentially Private Datasets Usi...</td>\n",
       "      <td>2018</td>\n",
       "      <td>In this paper, we present a technique for gene...</td>\n",
       "      <td>generative adversarial networks, differential ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Representing dynamically: An active process fo...</td>\n",
       "      <td>2018</td>\n",
       "      <td>We propose an unsupervised method for building...</td>\n",
       "      <td>Generative Models, Latent representations, Pre...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title  year  \\\n",
       "0   1       Hierarchical Adversarially Learned Inference  2018   \n",
       "1   2     Learning to Compute Word Embeddings On the Fly  2018   \n",
       "2   3   Graph2Seq: Scalable Learning Dynamics for Graphs  2018   \n",
       "3   4  Generating Differentially Private Datasets Usi...  2018   \n",
       "4   5  Representing dynamically: An active process fo...  2018   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  We propose a novel hierarchical generative mod...   \n",
       "1  Words in natural language follow a Zipfian dis...   \n",
       "2  Neural networks are increasingly used as a gen...   \n",
       "3  In this paper, we present a technique for gene...   \n",
       "4  We propose an unsupervised method for building...   \n",
       "\n",
       "                                            keywords  y  \n",
       "0  generative, hierarchical, unsupervised, semisu...  0  \n",
       "1      NLU, word embeddings, representation learning  0  \n",
       "2                                                NaN  0  \n",
       "3  generative adversarial networks, differential ...  0  \n",
       "4  Generative Models, Latent representations, Pre...  0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4974, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>abstract</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>StyleAlign: Analysis and Applications of Align...</td>\n",
       "      <td>2022</td>\n",
       "      <td>In this paper, we perform an in-depth study of...</td>\n",
       "      <td>StyleGAN, transfer learning, fine tuning, mode...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Embedding a random graph via GNN: mean-field i...</td>\n",
       "      <td>2021</td>\n",
       "      <td>We develop a theory for embedding a random gra...</td>\n",
       "      <td>Graph neural network, graph embedding, multi-r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>BBRefinement: an universal scheme to improve p...</td>\n",
       "      <td>2021</td>\n",
       "      <td>We present a conceptually simple yet powerful ...</td>\n",
       "      <td>object detection, deep neural networks, refine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Assessing Deep Reinforcement Learning Policies...</td>\n",
       "      <td>2022</td>\n",
       "      <td>Deep reinforcement learning algorithms have re...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Few-shot Learning with Big Prototypes</td>\n",
       "      <td>2022</td>\n",
       "      <td>Using dense vectors, i.e., prototypes, to repr...</td>\n",
       "      <td>Prototype, Few-shot Learning, Meta-learning</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title  year  \\\n",
       "0   1  StyleAlign: Analysis and Applications of Align...  2022   \n",
       "1   2  Embedding a random graph via GNN: mean-field i...  2021   \n",
       "2   3  BBRefinement: an universal scheme to improve p...  2021   \n",
       "3   4  Assessing Deep Reinforcement Learning Policies...  2022   \n",
       "4   5              Few-shot Learning with Big Prototypes  2022   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  In this paper, we perform an in-depth study of...   \n",
       "1  We develop a theory for embedding a random gra...   \n",
       "2  We present a conceptually simple yet powerful ...   \n",
       "3  Deep reinforcement learning algorithms have re...   \n",
       "4  Using dense vectors, i.e., prototypes, to repr...   \n",
       "\n",
       "                                            keywords  \n",
       "0  StyleGAN, transfer learning, fine tuning, mode...  \n",
       "1  Graph neural network, graph embedding, multi-r...  \n",
       "2  object detection, deep neural networks, refine...  \n",
       "3                                                NaN  \n",
       "4        Prototype, Few-shot Learning, Meta-learning  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6393, 5)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(os.path.join(INPUT_DIR, 'train_data.csv'))\n",
    "test = pd.read_csv(os.path.join(INPUT_DIR, 'test_data.csv'))\n",
    "submission_df = pd.read_csv(os.path.join(INPUT_DIR, 'submission.csv'))\n",
    "\n",
    "display(train.head())\n",
    "print(train.shape)\n",
    "display(test.head())\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=CFG.n_splits,shuffle=True,random_state=CFG.seed)\n",
    "for fold, ( _, val_) in enumerate(skf.split(train, train.y)):\n",
    "    train.loc[val_ , \"kfold\"] = int(fold)\n",
    "    \n",
    "train[\"kfold\"] = train[\"kfold\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "897204edcf9f47f6bd2501c0578cd41d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a26b676543b418fbede851eea18f720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/580 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "207326d1586d4c869f5a5fa58a278ade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model)\n",
    "tokenizer.save_pretrained(OUTPUT_MODEL_DIR+'tokenizer/')\n",
    "CFG.tokenizer = tokenizer\n",
    "SEP = tokenizer.sep_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length):\n",
    "        self.df = df\n",
    "        self.max_len = CFG.max_len\n",
    "        self.text = df['inputs'].values\n",
    "        self.tokenizer = CFG.tokenizer\n",
    "        self.targets = df['y'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.text[index]\n",
    "        inputs = tokenizer.encode_plus(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length = self.max_len\n",
    "        )\n",
    "        samples = {\n",
    "            'input_ids': inputs['input_ids'],\n",
    "            'attention_mask': inputs['attention_mask'],\n",
    "            'target': self.targets[index]\n",
    "        }\n",
    "\n",
    "        if 'token_type_ids' in inputs:\n",
    "            samples['token_type_ids'] = inputs['token_type_ids']\n",
    "            \n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probspace-xS3fZVNL-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
