{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold,StratifiedGroupKFold,GroupKFold,train_test_split\n",
    "from sklearn.metrics import log_loss,f1_score,accuracy_score\n",
    "\n",
    "from transformers import AutoModel, AutoConfig, AutoTokenizer, AdamW, DataCollatorWithPadding\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    wandb = False\n",
    "    apex = True\n",
    "    model = 'microsoft/deberta-v3-large'\n",
    "    seed = 42\n",
    "    n_splits = 5\n",
    "    max_len = 512\n",
    "    dropout = 0.2\n",
    "    target_size=4\n",
    "    n_accumulate=1\n",
    "    print_freq = 50\n",
    "    min_lr=1e-6\n",
    "    scheduler = 'cosine'\n",
    "    batch_size = 16\n",
    "    num_workers = 2\n",
    "    lr = 3e-5\n",
    "    weigth_decay = 0.01\n",
    "    epochs = 10\n",
    "    n_fold = 5\n",
    "    trn_fold = [0, 1, 2, 3, 4]\n",
    "    train = True \n",
    "    num_warmup_steps = 0\n",
    "    num_cycles=0.5\n",
    "    debug = False\n",
    "    debug_ver2 = False\n",
    "    gradient_checkpointing = True\n",
    "    freezing = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "INPUT_DIR = '/workspace/data/'\n",
    "OUTPUT_DIR = '/workspace/data/output/'\n",
    "OUTPUT_SUB_DIR = os.path.join(OUTPUT_DIR,'Submission')\n",
    "OUTPUT_MODEL_DIR = os.path.join('/workspace/model',CFG.model.replace('/','-'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loss Func\n",
    "def criterion(outputs, labels):\n",
    "    return nn.CrossEntropyLoss()(outputs, labels)\n",
    "\n",
    "def softmax(z):\n",
    "    assert len(z.shape) == 2\n",
    "    s = np.max(z, axis=1)\n",
    "    s = s[:, np.newaxis] # necessary step to do broadcasting\n",
    "    e_x = np.exp(z - s)\n",
    "    div = np.sum(e_x, axis=1)\n",
    "    div = div[:, np.newaxis] # dito\n",
    "    return e_x / div\n",
    "\"\"\"\n",
    "def get_score(y_true, y_pred):\n",
    "    y_pred = softmax(y_pred)\n",
    "    score = log_loss(y_true, y_pred)\n",
    "    return round(score, 5)\n",
    "\"\"\"\n",
    "def get_score(outputs, labels):\n",
    "    outputs = F.softmax(torch.tensor(outputs)).numpy()\n",
    "    return accuracy_score(np.argmax(outputs,axis=1),labels)\n",
    "\n",
    "def get_logger(filename=OUTPUT_DIR+'train'):\n",
    "    from logging import getLogger, INFO, FileHandler, Formatter, StreamHandler\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = get_logger()\n",
    "\n",
    "def seed_everything(seed=CFG.seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=42)\n",
    "\n",
    "def prepare_input(cfg, text, text_2=None):\n",
    "    inputs = cfg.tokenizer(text, text_2,\n",
    "                           padding=\"max_length\",\n",
    "                           add_special_tokens=True,\n",
    "                           max_length=cfg.max_len,\n",
    "                           truncation=True)\n",
    "\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "        \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def freeze(module):\n",
    "    \"\"\"\n",
    "    Freezes module's parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    for parameter in module.parameters():\n",
    "        parameter.requires_grad = False\n",
    "        \n",
    "def get_freezed_parameters(module):\n",
    "    \"\"\"\n",
    "    Returns names of freezed parameters of the given module.\n",
    "    \"\"\"\n",
    "    \n",
    "    freezed_parameters = []\n",
    "    for name, parameter in module.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            freezed_parameters.append(name)\n",
    "            \n",
    "    return freezed_parameters\n",
    "\n",
    "def set_embedding_parameters_bits(embeddings_path, optim_bits=32):\n",
    "    \"\"\"\n",
    "    https://github.com/huggingface/transformers/issues/14819#issuecomment-1003427930\n",
    "    \"\"\"\n",
    "    \n",
    "    embedding_types = (\"word\", \"position\", \"token_type\")\n",
    "    for embedding_type in embedding_types:\n",
    "        attr_name = f\"{embedding_type}_embeddings\"\n",
    "        \n",
    "        if hasattr(embeddings_path, attr_name): \n",
    "            bnb.optim.GlobalOptimManager.get_instance().register_module_override(\n",
    "                getattr(embeddings_path, attr_name), 'weight', {'optim_bits': optim_bits}\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(INPUT_DIR, 'train_data.csv'))\n",
    "test = pd.read_csv(os.path.join(INPUT_DIR, 'test_data.csv'))\n",
    "submission_df = pd.read_csv(os.path.join(INPUT_DIR, 'submission.csv'))\n",
    "\n",
    "display(train.head())\n",
    "print(train.shape)\n",
    "display(test.head())\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=CFG.n_splits,shuffle=True,random_state=CFG.seed)\n",
    "for fold, ( _, val_) in enumerate(skf.split(train, train.y)):\n",
    "    train.loc[val_ , \"kfold\"] = int(fold)\n",
    "    \n",
    "train[\"kfold\"] = train[\"kfold\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model)\n",
    "tokenizer.save_pretrained(os.path.join(OUTPUT_MODEL_DIR,CFG.model.replace('/','-'))+'tokenizer/')\n",
    "CFG.tokenizer = tokenizer\n",
    "SEP = tokenizer.sep_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length):\n",
    "        self.df = df\n",
    "        self.max_len = CFG.max_len\n",
    "        self.text = df['abstract'].values\n",
    "        self.tokenizer = CFG.tokenizer\n",
    "        self.targets = df['y'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.text[index]\n",
    "        inputs = tokenizer.encode_plus(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length = self.max_len\n",
    "        )\n",
    "        samples = {\n",
    "            'input_ids': inputs['input_ids'],\n",
    "            'attention_mask': inputs['attention_mask'],\n",
    "            'target': self.targets[index]\n",
    "        }\n",
    "\n",
    "        if 'token_type_ids' in inputs:\n",
    "            samples['token_type_ids'] = inputs['token_type_ids']\n",
    "            \n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dynamic Padding (Collate)\n",
    "#collate_fn = DataCollatorWithPadding(tokenizer=CFG.tokenizer)\n",
    "class Collate:\n",
    "    def __init__(self, tokenizer, isTrain=True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.isTrain = isTrain\n",
    "        # self.args = args\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        output = dict()\n",
    "        output[\"input_ids\"] = [sample[\"input_ids\"] for sample in batch]\n",
    "        output[\"attention_mask\"] = [sample[\"attention_mask\"] for sample in batch]\n",
    "        if self.isTrain:\n",
    "            output[\"target\"] = [sample[\"target\"] for sample in batch]\n",
    "\n",
    "        # calculate max token length of this batch\n",
    "        batch_max = max([len(ids) for ids in output[\"input_ids\"]])\n",
    "\n",
    "        # add padding\n",
    "        if self.tokenizer.padding_side == \"right\":\n",
    "            output[\"input_ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"input_ids\"]]\n",
    "            output[\"attention_mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"attention_mask\"]]\n",
    "        else:\n",
    "            output[\"input_ids\"] = [(batch_max - len(s)) * [self.tokenizer.pad_token_id] + s for s in output[\"input_ids\"]]\n",
    "            output[\"attention_mask\"] = [(batch_max - len(s)) * [0] + s for s in output[\"attention_mask\"]]\n",
    "\n",
    "        # convert to tensors\n",
    "        output[\"input_ids\"] = torch.tensor(output[\"input_ids\"], dtype=torch.long)\n",
    "        output[\"attention_mask\"] = torch.tensor(output[\"attention_mask\"], dtype=torch.long)\n",
    "        if self.isTrain:\n",
    "            output[\"target\"] = torch.tensor(output[\"target\"], dtype=torch.long)\n",
    "\n",
    "        return output\n",
    "    \n",
    "collate_fn = Collate(CFG.tokenizer, isTrain=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "        \n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9) #\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Model\n",
    "# ====================================================\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(CustomModel, self).__init__()\n",
    "        # Header (fast or normal)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Gradient_checkpointing\n",
    "        if CFG.gradient_checkpointing:\n",
    "            (self.model).gradient_checkpointing_enable()\n",
    "        \n",
    "        # Freezing\n",
    "        if CFG.freezing:\n",
    "            # freezing embeddings and first 2 layers of encoder\n",
    "            freeze((self.model).embeddings)\n",
    "            freeze((self.model).encoder.layer[:2])\n",
    "            CFG.after_freezed_parameters = filter(lambda parameter: parameter.requires_grad, (self.model).parameters())\n",
    "        \n",
    "        self.config = AutoConfig.from_pretrained(model_name)\n",
    "        self.drop = nn.Dropout(p=CFG.dropout)\n",
    "        self.pooler = MeanPooling()\n",
    "        self.fc = nn.Linear(self.config.hidden_size, CFG.target_size)\n",
    "        \n",
    "    def forward(self, ids, mask):        \n",
    "        out = self.model(input_ids=ids, \n",
    "                         attention_mask=mask,\n",
    "                         output_hidden_states=False)\n",
    "        out = self.pooler(out.last_hidden_state, mask)\n",
    "        out = self.drop(out)\n",
    "        outputs = self.fc(out)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def asMinutes(s):\n",
    "    m = math.floor(s/60)\n",
    "    s -= m * 60\n",
    "    return \"%dm %ds\" % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "    if cfg.scheduler == 'linear':\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n",
    "        )\n",
    "    elif cfg.scheduler == 'cosine':\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n",
    "        )\n",
    "    return scheduler\n",
    "\n",
    "def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n",
    "    model.train()\n",
    "\n",
    "    dataset_size = 0\n",
    "    running_loss = 0\n",
    "\n",
    "    start = end = time.time()\n",
    "\n",
    "    for step, data in enumerate(dataloader):\n",
    "        ids = data['input_ids'].to(device, dtype=torch.long)\n",
    "        mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "        targets = data['target'].to(device, dtype=torch.long)\n",
    "\n",
    "        batch_size = ids.size(0)\n",
    "        \n",
    "        outputs = model(ids, mask)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        #accumulate\n",
    "        loss = loss / CFG.n_accumulate \n",
    "        loss.backward()\n",
    "        if (step +1) % CFG.n_accumulate == 0:\n",
    "            optimizer.step()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        dataset_size += batch_size\n",
    "\n",
    "        epoch_loss = running_loss / dataset_size\n",
    "\n",
    "        end = time.time()\n",
    "        \n",
    "        if step % CFG.print_freq == 0 or step == (len(dataloader)-1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  .format(epoch+1, step, len(dataloader), \n",
    "                          remain=timeSince(start, float(step+1)/len(dataloader))))\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    return epoch_loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def valid_one_epoch(model, dataloader, device, epoch):\n",
    "    model.eval()\n",
    "\n",
    "    dataset_size = 0\n",
    "    running_loss = 0\n",
    "\n",
    "    start = end = time.time()\n",
    "    pred = []\n",
    "\n",
    "    for step, data in enumerate(dataloader):\n",
    "        ids = data['input_ids'].to(device, dtype=torch.long)\n",
    "        mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "        targets = data['target'].to(device, dtype=torch.long)\n",
    "\n",
    "        batch_size = ids.size(0)\n",
    "        outputs = model(ids, mask)\n",
    "        loss = criterion(outputs, targets)\n",
    "        pred.append(outputs.to('cpu').numpy())\n",
    "\n",
    "        running_loss += (loss.item()* batch_size)\n",
    "        dataset_size += batch_size\n",
    "\n",
    "        epoch_loss = running_loss / dataset_size\n",
    "\n",
    "        end = time.time()\n",
    "\n",
    "        if step % CFG.print_freq == 0 or step == (len(dataloader)-1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  .format(step, len(dataloader),\n",
    "                          remain=timeSince(start, float(step+1)/len(dataloader))))\n",
    "            \n",
    "    pred = np.concatenate(pred)\n",
    "            \n",
    "    return epoch_loss, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_loop(fold):\n",
    "    #wandb.watch(model, log_freq=100)\n",
    "\n",
    "    LOGGER.info(f'-------------fold:{fold} training-------------')\n",
    "\n",
    "    train_data = train[train.kfold != fold].reset_index(drop=True)\n",
    "    valid_data = train[train.kfold == fold].reset_index(drop=True)\n",
    "    valid_labels = valid_data.y.values\n",
    "\n",
    "    trainDataset = Dataset(train_data, CFG.tokenizer, CFG.max_len)\n",
    "    validDataset = Dataset(valid_data, CFG.tokenizer, CFG.max_len)\n",
    "\n",
    "    train_loader = DataLoader(trainDataset,\n",
    "                              batch_size = CFG.batch_size,\n",
    "                              shuffle=True,\n",
    "                              collate_fn = collate_fn,\n",
    "                              num_workers = CFG.num_workers,\n",
    "                              pin_memory = True,\n",
    "                              drop_last=True)\n",
    "    \n",
    "    valid_loader = DataLoader(validDataset,\n",
    "                              batch_size = CFG.batch_size*2,\n",
    "                              shuffle=False,\n",
    "                              collate_fn = collate_fn,\n",
    "                              num_workers = CFG.num_workers,\n",
    "                              pin_memory = True,\n",
    "                              drop_last=False)\n",
    "    \n",
    "    model = CustomModel(CFG.model)\n",
    "    torch.save(model.config,OUTPUT_MODEL_DIR+'config.pth')\n",
    "    model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=CFG.lr, weight_decay=CFG.weigth_decay)\n",
    "    num_train_steps = int(len(train_data) / CFG.batch_size * CFG.epochs)\n",
    "    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n",
    "\n",
    "    # loop\n",
    "    best_score = 0\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_epoch_loss = train_one_epoch(model, optimizer, scheduler, train_loader, device, epoch)\n",
    "        valid_epoch_loss, pred = valid_one_epoch(model, valid_loader, device, epoch)\n",
    "\n",
    "        score = get_score(pred, valid_labels)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {train_epoch_loss:.4f}  avg_val_loss: {valid_epoch_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}')\n",
    "        if CFG.wandb:\n",
    "            wandb.log({f\"[fold{fold}] epoch\": epoch+1, \n",
    "                       f\"[fold{fold}] avg_train_loss\": train_epoch_loss, \n",
    "                       f\"[fold{fold}] avg_val_loss\": valid_epoch_loss,\n",
    "                       f\"[fold{fold}] score\": score})\n",
    "            \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save({'model': model.state_dict(),\n",
    "                        'predictions': pred},\n",
    "                        OUTPUT_MODEL_DIR+f\"_fold{fold}_best.pth\")\n",
    "            \n",
    "    predictions = torch.load(OUTPUT_MODEL_DIR+f\"_fold{fold}_best.pth\", \n",
    "                             map_location=torch.device('cpu'))['predictions']\n",
    "    valid_data['0'] = predictions[:, 0]\n",
    "    valid_data['1'] = predictions[:, 1]\n",
    "    \n",
    "    \n",
    "    temp = valid_data[['0','1']].values.tolist()\n",
    "    print(get_score(temp, valid_data['y'].values))\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "CFG.epochs = 10\n",
    "CFG.max_len = 5\n",
    "\n",
    "12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    def get_result(oof_df):\n",
    "        labels = oof_df['y'].values\n",
    "        preds = oof_df[['0','1']].values.tolist()\n",
    "        score = get_score(preds, labels)\n",
    "        LOGGER.info(f'Score: {score:<.4f}')\n",
    "    \n",
    "    if CFG.train:\n",
    "        oof_df = pd.DataFrame()\n",
    "        for fold in range(CFG.n_fold):\n",
    "            if fold in CFG.trn_fold:\n",
    "                _oof_df = train_loop(fold)\n",
    "                oof_df = pd.concat([oof_df, _oof_df])\n",
    "                LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "                get_result(_oof_df)\n",
    "        # _oof_df = train_loop(0)\n",
    "        # oof_df = pd.concat([oof_df, _oof_df])\n",
    "        # LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "        # get_result(_oof_df)\n",
    "        # oof_df = oof_df.reset_index(drop=True)\n",
    "        # LOGGER.info(f\"========== CV ==========\")\n",
    "        # get_result(oof_df)\n",
    "        oof_df.to_pickle(OUTPUT_MODEL_DIR+'oof_df.pkl')\n",
    "        oof_df.to_csv(OUTPUT_MODEL_DIR+f'oof_df.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
