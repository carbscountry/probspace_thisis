{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"c31fc42098294655bd78b919ffa52746":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e6d0660fa0404071bcf92fbb4616c718","IPY_MODEL_cb91cc808b5447ef895e41723bfb5e9f","IPY_MODEL_bb3d04dc4de247c88e7ed9ffbeb8921d"],"layout":"IPY_MODEL_b5dc6274733d46379530fc003c1c0dcc"}},"e6d0660fa0404071bcf92fbb4616c718":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eea47f92196f4bf4adc8de37b5888be0","placeholder":"​","style":"IPY_MODEL_d3c49de147044ee6ba455986fec97439","value":"Downloading (…)olve/main/vocab.json: 100%"}},"cb91cc808b5447ef895e41723bfb5e9f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fe44a07eb69b4bbfb13e9131f5ed90d7","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_05b411300cca41a4a414dd125a34de8e","value":898823}},"bb3d04dc4de247c88e7ed9ffbeb8921d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_82b49ba159e64ee48e2e2cb14a9e9bb4","placeholder":"​","style":"IPY_MODEL_375816d89b194cb8ab63ea48a905cdcc","value":" 899k/899k [00:01&lt;00:00, 777kB/s]"}},"b5dc6274733d46379530fc003c1c0dcc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eea47f92196f4bf4adc8de37b5888be0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3c49de147044ee6ba455986fec97439":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fe44a07eb69b4bbfb13e9131f5ed90d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"05b411300cca41a4a414dd125a34de8e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"82b49ba159e64ee48e2e2cb14a9e9bb4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"375816d89b194cb8ab63ea48a905cdcc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"618bd2924fb642e4a5a1314cdc038f2c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b40e25b111764141b33bcdaf302080b5","IPY_MODEL_495a145368db4bfc90408bd1c7c827d7","IPY_MODEL_19909fbc9f6d4db4bdaf1d7ea64512c7"],"layout":"IPY_MODEL_935cf256af334de2bc78ebb91a1f9b9b"}},"b40e25b111764141b33bcdaf302080b5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_94c8cd8a911941c580c37e8708495d89","placeholder":"​","style":"IPY_MODEL_32f5013a794e4a8cb142b9d7763dd446","value":"Downloading (…)olve/main/merges.txt: 100%"}},"495a145368db4bfc90408bd1c7c827d7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_45e0ecea37f4405686e0842ec143a54e","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6bb0ad9249c54067bea7a97e927ad984","value":456318}},"19909fbc9f6d4db4bdaf1d7ea64512c7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e38188d6795744e49bcae696b6c8b29b","placeholder":"​","style":"IPY_MODEL_ea855f009e8d4a08ae7874b25737cf57","value":" 456k/456k [00:01&lt;00:00, 413kB/s]"}},"935cf256af334de2bc78ebb91a1f9b9b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"94c8cd8a911941c580c37e8708495d89":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32f5013a794e4a8cb142b9d7763dd446":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"45e0ecea37f4405686e0842ec143a54e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6bb0ad9249c54067bea7a97e927ad984":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e38188d6795744e49bcae696b6c8b29b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea855f009e8d4a08ae7874b25737cf57":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a6b201a1981d4148a443e4ae87cc7288":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b5a4c24c4b7a4b2d9c52acb3acfc4ba7","IPY_MODEL_6cefb1bfda94418785f3d9799f7bf721","IPY_MODEL_8e1ac1293408448d80407ff508a3c423"],"layout":"IPY_MODEL_8142ddad0d5a43fb8028a4b9278814c4"}},"b5a4c24c4b7a4b2d9c52acb3acfc4ba7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cf3a883a729f4c78a3bf21a24b3b437e","placeholder":"​","style":"IPY_MODEL_9e8cb7219b36498a9f0429ff8d987321","value":"Downloading (…)lve/main/config.json: 100%"}},"6cefb1bfda94418785f3d9799f7bf721":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_340a891b82194a12bff5ba2ff5e8c1a3","max":481,"min":0,"orientation":"horizontal","style":"IPY_MODEL_620ec167ab7a44e080b2ae3264a26ed9","value":481}},"8e1ac1293408448d80407ff508a3c423":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e2ba1321a505484fba745eada766a282","placeholder":"​","style":"IPY_MODEL_c67ed91fa54a4d74a155bb48c7985762","value":" 481/481 [00:00&lt;00:00, 24.5kB/s]"}},"8142ddad0d5a43fb8028a4b9278814c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf3a883a729f4c78a3bf21a24b3b437e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9e8cb7219b36498a9f0429ff8d987321":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"340a891b82194a12bff5ba2ff5e8c1a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"620ec167ab7a44e080b2ae3264a26ed9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e2ba1321a505484fba745eada766a282":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c67ed91fa54a4d74a155bb48c7985762":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"55c6e02b79884ba8b751a68663ca1344":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_94b91c8cbff742ccaf9605300656352d","IPY_MODEL_6613c7527aab4777a140b5730c6e34d5","IPY_MODEL_3d42e797313446fcbaf69bbbf3fcf340"],"layout":"IPY_MODEL_29b1caae2002483b976b2c09ecd33e16"}},"94b91c8cbff742ccaf9605300656352d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_27d9706c0189450383fc17590ff769ad","placeholder":"​","style":"IPY_MODEL_fc6fcab3b8fd4afa8e0d978952f59afa","value":"Downloading (…)&quot;pytorch_model.bin&quot;;: 100%"}},"6613c7527aab4777a140b5730c6e34d5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_75738d969fb040d4a24aa8abf382c5c0","max":501200538,"min":0,"orientation":"horizontal","style":"IPY_MODEL_43b6b198f7134e399c8625d62ad2fa49","value":501200538}},"3d42e797313446fcbaf69bbbf3fcf340":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c450cf426aa414dac4f603d2911ea1b","placeholder":"​","style":"IPY_MODEL_7f9b39e4169b454bb13516cb724fcbca","value":" 501M/501M [00:01&lt;00:00, 284MB/s]"}},"29b1caae2002483b976b2c09ecd33e16":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"27d9706c0189450383fc17590ff769ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc6fcab3b8fd4afa8e0d978952f59afa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"75738d969fb040d4a24aa8abf382c5c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"43b6b198f7134e399c8625d62ad2fa49":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5c450cf426aa414dac4f603d2911ea1b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f9b39e4169b454bb13516cb724fcbca":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# BERT Base line−コメント付き (LB 0.7387 ) by Oregin\n","研究論文の国際学会採択予測で、件名と概要を利用した簡単なBERTのサンプルコードです。まだ、BERTについて勉強し始めたばかりなので、精度や見栄えはイマイチですが、なんとか動くものを作れました。\n","ご参考までご活用ください。\n","\n","※Google Colab（GPU利用）で実行可能です。\n","\n","LB= 0.7387 でした。\n","\n","ディレクトリ構成\n","- base_path : このファイルを入れておくディレクトリ（各種パスの設定にて、保存した絶対パスを指定してください）\n","- base_path/data : test_data.csv,train_data.csv,submission.csvを入れておくディレクトリ\n","- base_path/model : 学習済みのモデルを保存するディレクトリ\n","- base_path/output : 提出用ファイルを保存するディレクトリ\n"],"metadata":{"id":"LrBMtxoUMzyI"}},{"cell_type":"markdown","source":["\n","\n","# ライブラリのインポート\n"],"metadata":{"id":"BVIeIq2quBRa"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('./drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QMipctm0Lxnu","executionInfo":{"status":"ok","timestamp":1677305928307,"user_tz":-540,"elapsed":2028,"user":{"displayName":"塚田真輝","userId":"12257735310978153438"}},"outputId":"4b844f6c-bb37-42c0-8746-4966372cadc7"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at ./drive; to attempt to forcibly remount, call drive.mount(\"./drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"Zq04UNDZ6dtr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677305937786,"user_tz":-540,"elapsed":5813,"user":{"displayName":"塚田真輝","userId":"12257735310978153438"}},"outputId":"88b63998-530e-4dfc-aa92-bba55f835500"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.26.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.12.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n"]}]},{"cell_type":"code","source":["# 各種ライブラリのインポート\n","import os\n","import pandas as pd\n","import itertools\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torch.nn.utils.rnn import pad_sequence\n","from transformers import BertModel, BertTokenizer\n","\n","from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n","from sklearn.model_selection import train_test_split\n","from tqdm import tqdm\n","import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"CjawPcQPfSZm","executionInfo":{"status":"ok","timestamp":1677305986910,"user_tz":-540,"elapsed":1,"user":{"displayName":"塚田真輝","userId":"12257735310978153438"}}},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":["# 各種パスの設定"],"metadata":{"id":"RyxQoDTDVfrH"}},{"cell_type":"code","source":["# 各種パスの設定\n","######################################################################\n","base_path = '/content/drive/MyDrive/kaggle/probspace' # ベースとなるパスを指定してください。#######\n","######################################################################\n","os.makedirs(os.path.join(base_path,'model'), exist_ok=True)  # 学習済みモデルの保存するディレクトリを作成\n","os.makedirs(os.path.join(base_path,'output'), exist_ok=True)  # 提出用ファイルを出力するディレクトリを作成\n","train_data_path = os.path.join(base_path,'data/train_data.csv') # 訓練データのパスを指定\n","test_data_path = os.path.join(base_path,'data/test_data.csv') # テストデータのパスを指定\n","submit_data_path = os.path.join(base_path,'data/submission.csv') # 提出用サンプルfileのパスを指定\n","model_file_path = os.path.join(base_path,'model/best_model.pt') # 学習済みモデルのパスを指定\n","output_file_path = os.path.join(base_path,'output/001_submission.csv') # 提出用ファイルのパスを指定"],"metadata":{"id":"xhqjpkeqU9m4","executionInfo":{"status":"ok","timestamp":1677305942186,"user_tz":-540,"elapsed":1,"user":{"displayName":"塚田真輝","userId":"12257735310978153438"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["# 各種データの読み込み\n","\n"],"metadata":{"id":"6mdDx9jLVj7N"}},{"cell_type":"code","source":["# 各種データの読込\n","df = pd.read_csv(train_data_path) # 訓練データの読込\n","test_df = pd.read_csv(test_data_path) # テストデータの読込\n","test_df['y'] = 0 # テストデータのＹの値を初期化\n","submit_df = pd.read_csv(submit_data_path) # 提出用散布リファイルの読込"],"metadata":{"id":"n2YAsmQ5fVBd","executionInfo":{"status":"ok","timestamp":1677305944670,"user_tz":-540,"elapsed":2,"user":{"displayName":"塚田真輝","userId":"12257735310978153438"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["# 関数の定義"],"metadata":{"id":"a006Kjw3oukP"}},{"cell_type":"code","source":["# BERTを使用してトークナイズするためのクラスを定義します。\n","class BERTTokenize(nn.Module):\n","    def __init__(self):\n","        super(BERTTokenize, self).__init__()\n","        self.tokenizer =RobertaTokenizer.from_pretrained('roberta-base')\n","\n","    def forward(self, text):\n","        return self.tokenizer.encode_plus(\n","            text,\n","            max_length=512,\n","            add_special_tokens=True,\n","            return_token_type_ids=False,\n","            padding='max_length',\n","            return_attention_mask=True,\n","            return_tensors='pt',\n","            truncation=True\n","        )\n"],"metadata":{"id":"l26LRH31fXN1","executionInfo":{"status":"ok","timestamp":1677306002205,"user_tz":-540,"elapsed":424,"user":{"displayName":"塚田真輝","userId":"12257735310978153438"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["#　データセットを作成するクラスを定義します。\n","class PaperDataset(Dataset):\n","    def __init__(self, dataframe, tokenizer):\n","        self.data = dataframe\n","        self.tokenizer = tokenizer\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        title = self.data.iloc[index]['title']\n","        abstract = self.data.iloc[index]['abstract']\n","        text = title + ' ' + abstract\n","        label = self.data.iloc[index]['y']\n","        inputs = self.tokenizer(text)\n","        return {\n","            'input_ids': inputs['input_ids'].flatten(),\n","            'attention_mask': inputs['attention_mask'].flatten(),\n","            'targets': torch.tensor(label, dtype=torch.float)\n","        }\n"],"metadata":{"id":"sH25akopfZYN","executionInfo":{"status":"ok","timestamp":1677306005519,"user_tz":-540,"elapsed":2,"user":{"displayName":"塚田真輝","userId":"12257735310978153438"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["# datasetで定義されたバッチ形式の__getitem__を処理して、オリジナルのtorch.Tensorにする関数\n","def collate_fn(batch):\n","    input_ids = pad_sequence([torch.tensor(item[\"input_ids\"]) for item in batch], batch_first=True)\n","    attention_mask = pad_sequence([torch.tensor(item[\"attention_mask\"]) for item in batch], batch_first=True)\n","    labels = torch.tensor([item['targets'] for item in batch])\n","    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, 'targets': labels}\n"],"metadata":{"id":"GVj0_FbOiw7c","executionInfo":{"status":"ok","timestamp":1677306008301,"user_tz":-540,"elapsed":433,"user":{"displayName":"塚田真輝","userId":"12257735310978153438"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["#　BERTのモデルのクラスを定義します。\n","class BERTModel(nn.Module):\n","    def __init__(self):\n","        super(BERTModel, self).__init__()\n","        self.bert = BertModel.from_pretrained('roberta-base')\n","        self.dropout = nn.Dropout(0.3)\n","        self.fc = nn.Linear(768, 1)\n","        self.sig = nn.Sigmoid()\n","\n","    def forward(self, input_ids, attention_mask):\n","        outputs = self.bert(input_ids, attention_mask=attention_mask,token_type_ids=None)\n","        pooled_output = outputs[1]\n","        dropout_output = self.dropout(pooled_output)\n","        output = self.fc(dropout_output)\n","        return self.sig(output)\n"],"metadata":{"id":"AGa5FNTPff6O","executionInfo":{"status":"ok","timestamp":1677306009380,"user_tz":-540,"elapsed":1,"user":{"displayName":"塚田真輝","userId":"12257735310978153438"}}},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":["# データローダの作成"],"metadata":{"id":"9YimcHviqNoU"}},{"cell_type":"code","source":["#トレーニングとバリデーションのためのデータローダーを作成します。\n","train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n","\n","tokenizer = BERTTokenize()\n","\n","train_dataset = PaperDataset(train_df, tokenizer)\n","val_dataset = PaperDataset(val_df, tokenizer)\n","\n","train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n","val_loader = DataLoader(val_dataset, batch_size=8, collate_fn=collate_fn)"],"metadata":{"id":"smaQQxoVfdBZ","colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["c31fc42098294655bd78b919ffa52746","e6d0660fa0404071bcf92fbb4616c718","cb91cc808b5447ef895e41723bfb5e9f","bb3d04dc4de247c88e7ed9ffbeb8921d","b5dc6274733d46379530fc003c1c0dcc","eea47f92196f4bf4adc8de37b5888be0","d3c49de147044ee6ba455986fec97439","fe44a07eb69b4bbfb13e9131f5ed90d7","05b411300cca41a4a414dd125a34de8e","82b49ba159e64ee48e2e2cb14a9e9bb4","375816d89b194cb8ab63ea48a905cdcc","618bd2924fb642e4a5a1314cdc038f2c","b40e25b111764141b33bcdaf302080b5","495a145368db4bfc90408bd1c7c827d7","19909fbc9f6d4db4bdaf1d7ea64512c7","935cf256af334de2bc78ebb91a1f9b9b","94c8cd8a911941c580c37e8708495d89","32f5013a794e4a8cb142b9d7763dd446","45e0ecea37f4405686e0842ec143a54e","6bb0ad9249c54067bea7a97e927ad984","e38188d6795744e49bcae696b6c8b29b","ea855f009e8d4a08ae7874b25737cf57","a6b201a1981d4148a443e4ae87cc7288","b5a4c24c4b7a4b2d9c52acb3acfc4ba7","6cefb1bfda94418785f3d9799f7bf721","8e1ac1293408448d80407ff508a3c423","8142ddad0d5a43fb8028a4b9278814c4","cf3a883a729f4c78a3bf21a24b3b437e","9e8cb7219b36498a9f0429ff8d987321","340a891b82194a12bff5ba2ff5e8c1a3","620ec167ab7a44e080b2ae3264a26ed9","e2ba1321a505484fba745eada766a282","c67ed91fa54a4d74a155bb48c7985762"]},"executionInfo":{"status":"ok","timestamp":1677306058221,"user_tz":-540,"elapsed":10890,"user":{"displayName":"塚田真輝","userId":"12257735310978153438"}},"outputId":"7f7038c8-159e-44de-e5aa-2cb5222b4290"},"execution_count":33,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c31fc42098294655bd78b919ffa52746"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"618bd2924fb642e4a5a1314cdc038f2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6b201a1981d4148a443e4ae87cc7288"}},"metadata":{}}]},{"cell_type":"markdown","source":["# モデルの作成"],"metadata":{"id":"zTcNbALLqg6q"}},{"cell_type":"code","source":["# 損失関数とオプティマイザーを定義して、モデルを作成します。\n","model = BERTModel()\n","criterion = nn.BCELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)"],"metadata":{"id":"HfL2gdqHfhwR","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["55c6e02b79884ba8b751a68663ca1344","94b91c8cbff742ccaf9605300656352d","6613c7527aab4777a140b5730c6e34d5","3d42e797313446fcbaf69bbbf3fcf340","29b1caae2002483b976b2c09ecd33e16","27d9706c0189450383fc17590ff769ad","fc6fcab3b8fd4afa8e0d978952f59afa","75738d969fb040d4a24aa8abf382c5c0","43b6b198f7134e399c8625d62ad2fa49","5c450cf426aa414dac4f603d2911ea1b","7f9b39e4169b454bb13516cb724fcbca"]},"executionInfo":{"status":"ok","timestamp":1677306069357,"user_tz":-540,"elapsed":6131,"user":{"displayName":"塚田真輝","userId":"12257735310978153438"}},"outputId":"583541ea-db71-4365-fb9f-06fdfb388427"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stderr","text":["You are using a model of type roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/501M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55c6e02b79884ba8b751a68663ca1344"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing BertModel: ['roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.self.value.bias', 'lm_head.dense.weight', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'lm_head.layer_norm.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'lm_head.decoder.weight', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.embeddings.word_embeddings.weight', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'lm_head.dense.bias', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.pooler.dense.weight', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.embeddings.position_embeddings.weight', 'lm_head.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.pooler.dense.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.embeddings.LayerNorm.weight', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.6.intermediate.dense.weight', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.5.attention.self.key.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['encoder.layer.10.intermediate.dense.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.7.attention.self.value.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'pooler.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.0.attention.self.query.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.2.intermediate.dense.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.2.output.dense.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.weight', 'pooler.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.4.output.LayerNorm.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["BERTModel(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n","      (position_embeddings): Embedding(514, 768)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.3, inplace=False)\n","  (fc): Linear(in_features=768, out_features=1, bias=True)\n","  (sig): Sigmoid()\n",")"]},"metadata":{},"execution_count":34}]},{"cell_type":"markdown","source":["# 学習の実行"],"metadata":{"id":"M-pgKHlYqvdn"}},{"cell_type":"code","source":["# 学習の関数の定義\n","def train(model, dataloader, optimizer, criterion):\n","    model.train()\n","    train_loss = 0\n","    for batch in dataloader:\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        targets = batch['targets'].view(-1, 1).to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(input_ids, attention_mask)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","\n","    return train_loss / len(dataloader)\n","# 検証の関数の定義\n","def validate(model, dataloader, criterion):\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for batch in dataloader:\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            targets = batch['targets'].view(-1, 1).to(device)\n","\n","            outputs = model(input_ids, attention_mask)\n","            loss = criterion(outputs, targets)\n","\n","            val_loss += loss.item()\n","\n","    return val_loss / len(dataloader)\n"],"metadata":{"id":"DTSfJdIeqtq5","executionInfo":{"status":"ok","timestamp":1677306073362,"user_tz":-540,"elapsed":429,"user":{"displayName":"塚田真輝","userId":"12257735310978153438"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["#　学習・検証の実行\n","\n","best_val_loss = float('inf')\n","\n","for epoch in tqdm(range(10)):\n","    train_loss = train(model, train_loader, optimizer, criterion)\n","    val_loss = validate(model, val_loader, criterion)\n","\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        torch.save(model.state_dict(), model_file_path) # ベストなモデルを保存する\n","\n","    print(f'Epoch {epoch+1} - train loss: {train_loss:.3f}, val loss: {val_loss:.3f}')\n"],"metadata":{"id":"6M38BbiDfkz9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677310142309,"user_tz":-540,"elapsed":4065210,"user":{"displayName":"塚田真輝","userId":"12257735310978153438"}},"outputId":"4a20cdd1-3860-4c06-dae6-6b3aa8a6d852"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stderr","text":[" 10%|█         | 1/10 [06:50<1:01:31, 410.18s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 1 - train loss: 0.628, val loss: 0.618\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 2/10 [13:37<54:29, 408.63s/it]  "]},{"output_type":"stream","name":"stdout","text":["Epoch 2 - train loss: 0.627, val loss: 0.614\n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|███       | 3/10 [20:23<47:32, 407.45s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 3 - train loss: 0.625, val loss: 0.617\n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|████      | 4/10 [27:09<40:40, 406.69s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 4 - train loss: 0.626, val loss: 0.615\n"]},{"output_type":"stream","name":"stderr","text":["\r 50%|█████     | 5/10 [33:55<33:52, 406.59s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 5 - train loss: 0.629, val loss: 0.615\n"]},{"output_type":"stream","name":"stderr","text":["\r 60%|██████    | 6/10 [40:41<27:04, 406.23s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 6 - train loss: 0.625, val loss: 0.615\n"]},{"output_type":"stream","name":"stderr","text":["\r 70%|███████   | 7/10 [47:26<20:17, 405.85s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 7 - train loss: 0.626, val loss: 0.615\n"]},{"output_type":"stream","name":"stderr","text":["\r 80%|████████  | 8/10 [54:11<13:31, 405.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 8 - train loss: 0.627, val loss: 0.614\n"]},{"output_type":"stream","name":"stderr","text":["\r 90%|█████████ | 9/10 [1:00:56<06:45, 405.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 9 - train loss: 0.624, val loss: 0.615\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [1:07:45<00:00, 406.53s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 10 - train loss: 0.625, val loss: 0.614\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["# テストデータで予測する"],"metadata":{"id":"2yRwxNapDp_2"}},{"cell_type":"code","source":["# モデルの読み込み\n","model.load_state_dict(torch.load(model_file_path))"],"metadata":{"id":"WlUyZDPKPpF8","executionInfo":{"status":"ok","timestamp":1677310143354,"user_tz":-540,"elapsed":1049,"user":{"displayName":"塚田真輝","userId":"12257735310978153438"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"683aff87-b5a0-4e16-8282-eab6edb67bfb"},"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":["#　テストデータのデータローダ―の作成\n","tokenizer = BERTTokenize()\n","\n","test_dataset = PaperDataset(test_df, tokenizer)\n","test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False,collate_fn=collate_fn)"],"metadata":{"id":"swmzV_MmEOM4","executionInfo":{"status":"ok","timestamp":1677310144453,"user_tz":-540,"elapsed":1101,"user":{"displayName":"塚田真輝","userId":"12257735310978153438"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["#　予測する関数の定義\n","def predict(model, dataloader):\n","  outputs = []\n","  with torch.no_grad():\n","    for batch in dataloader:\n","      input_ids = batch['input_ids'].to(device)\n","      attention_mask = batch['attention_mask'].to(device)\n","      # モデルによる予測\n","      output = model(input_ids, attention_mask=attention_mask)\n","      outputs.extend(list(itertools.chain.from_iterable(output.tolist())))\n","            \n","  return outputs\n","\n","#予測の実行\n","outputs = predict(model, test_loader)"],"metadata":{"id":"J7JF4GFKE6EU","executionInfo":{"status":"ok","timestamp":1677310352136,"user_tz":-540,"elapsed":207686,"user":{"displayName":"塚田真輝","userId":"12257735310978153438"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["#予測結果を提出形式に合わせて出力\n","submit_df['y'] = pd.Series(outputs)\n","submit_df['y'].mask(submit_df['y'] > 0.5,1,inplace=True)\n","submit_df['y'].mask(submit_df['y'] < 1,0,inplace=True)\n","submit_df['y'] = submit_df['y'].astype('int')\n","submit_df.to_csv(output_file_path,index = False)"],"metadata":{"id":"moXeNAlCIeqR","executionInfo":{"status":"ok","timestamp":1677310352137,"user_tz":-540,"elapsed":20,"user":{"displayName":"塚田真輝","userId":"12257735310978153438"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["outputs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":175},"id":"0Jn6pDr5fc3S","executionInfo":{"status":"error","timestamp":1677310833297,"user_tz":-540,"elapsed":454,"user":{"displayName":"塚田真輝","userId":"12257735310978153438"}},"outputId":"d7a1072b-3491-44e6-e6a6-98fd0cb91c2b"},"execution_count":46,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-46-9e1bc36f85cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'describe'"]}]},{"cell_type":"code","source":["sorted(outputs, reverse=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rdXyLSIRfeic","executionInfo":{"status":"ok","timestamp":1677310905705,"user_tz":-540,"elapsed":4,"user":{"displayName":"塚田真輝","userId":"12257735310978153438"}},"outputId":"5f4dfc4a-2dbc-4343-d428-1c4a8eedf257"},"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.3075016736984253,\n"," 0.30750054121017456,\n"," 0.3074963390827179,\n"," 0.30749472975730896,\n"," 0.30749019980430603,\n"," 0.30748817324638367,\n"," 0.30748632550239563,\n"," 0.3074854016304016,\n"," 0.30748313665390015,\n"," 0.30748289823532104,\n"," 0.3074825704097748,\n"," 0.3074825704097748,\n"," 0.3074823021888733,\n"," 0.3074807822704315,\n"," 0.3074793815612793,\n"," 0.30747926235198975,\n"," 0.307478666305542,\n"," 0.30747827887535095,\n"," 0.3074781894683838,\n"," 0.3074771761894226,\n"," 0.3074769079685211,\n"," 0.3074764907360077,\n"," 0.3074764609336853,\n"," 0.3074756860733032,\n"," 0.3074747622013092,\n"," 0.3074740469455719,\n"," 0.30747273564338684,\n"," 0.3074725568294525,\n"," 0.30747246742248535,\n"," 0.3074723482131958,\n"," 0.30747196078300476,\n"," 0.3074715733528137,\n"," 0.307471364736557,\n"," 0.3074711859226227,\n"," 0.30747100710868835,\n"," 0.30747079849243164,\n"," 0.30747032165527344,\n"," 0.30747032165527344,\n"," 0.3074702322483063,\n"," 0.30747002363204956,\n"," 0.30746990442276,\n"," 0.3074689507484436,\n"," 0.30746886134147644,\n"," 0.3074687123298645,\n"," 0.30746856331825256,\n"," 0.3074684739112854,\n"," 0.30746838450431824,\n"," 0.3074682056903839,\n"," 0.3074679374694824,\n"," 0.30746760964393616,\n"," 0.307467520236969,\n"," 0.3074674606323242,\n"," 0.30746737122535706,\n"," 0.307466983795166,\n"," 0.3074668049812317,\n"," 0.30746567249298096,\n"," 0.3074656128883362,\n"," 0.30746543407440186,\n"," 0.3074653744697571,\n"," 0.3074653446674347,\n"," 0.3074645400047302,\n"," 0.30746451020240784,\n"," 0.3074644207954407,\n"," 0.3074643909931183,\n"," 0.30746424198150635,\n"," 0.30746421217918396,\n"," 0.3074641227722168,\n"," 0.30746394395828247,\n"," 0.3074638843536377,\n"," 0.3074635863304138,\n"," 0.30746355652809143,\n"," 0.30746352672576904,\n"," 0.3074634373188019,\n"," 0.30746304988861084,\n"," 0.3074626624584198,\n"," 0.3074626624584198,\n"," 0.3074624836444855,\n"," 0.30746227502822876,\n"," 0.3074619770050049,\n"," 0.30746182799339294,\n"," 0.3074617385864258,\n"," 0.30746155977249146,\n"," 0.3074614107608795,\n"," 0.30746129155158997,\n"," 0.3074612617492676,\n"," 0.3074611723423004,\n"," 0.30746108293533325,\n"," 0.30746105313301086,\n"," 0.3074609935283661,\n"," 0.3074609041213989,\n"," 0.3074607849121094,\n"," 0.3074606955051422,\n"," 0.30746060609817505,\n"," 0.30746057629585266,\n"," 0.30746036767959595,\n"," 0.3074602782726288,\n"," 0.307460218667984,\n"," 0.30746012926101685,\n"," 0.3074600398540497,\n"," 0.3074597120285034,\n"," 0.30745965242385864,\n"," 0.30745962262153625,\n"," 0.30745959281921387,\n"," 0.30745935440063477,\n"," 0.3074593245983124,\n"," 0.30745914578437805,\n"," 0.30745914578437805,\n"," 0.30745914578437805,\n"," 0.30745887756347656,\n"," 0.3074588179588318,\n"," 0.3074587881565094,\n"," 0.3074585795402527,\n"," 0.3074585795402527,\n"," 0.30745840072631836,\n"," 0.3074583411216736,\n"," 0.3074583411216736,\n"," 0.3074583411216736,\n"," 0.30745816230773926,\n"," 0.30745771527290344,\n"," 0.30745771527290344,\n"," 0.3074575364589691,\n"," 0.30745750665664673,\n"," 0.30745747685432434,\n"," 0.30745723843574524,\n"," 0.3074571490287781,\n"," 0.3074571192264557,\n"," 0.3074570894241333,\n"," 0.3074570596218109,\n"," 0.30745700001716614,\n"," 0.307456910610199,\n"," 0.3074568510055542,\n"," 0.3074567914009094,\n"," 0.3074566721916199,\n"," 0.3074566721916199,\n"," 0.3074566423892975,\n"," 0.3074566423892975,\n"," 0.3074565529823303,\n"," 0.30745652318000793,\n"," 0.30745643377304077,\n"," 0.307456374168396,\n"," 0.307456374168396,\n"," 0.30745619535446167,\n"," 0.30745619535446167,\n"," 0.30745619535446167,\n"," 0.3074561059474945,\n"," 0.30745601654052734,\n"," 0.30745595693588257,\n"," 0.30745595693588257,\n"," 0.30745580792427063,\n"," 0.30745574831962585,\n"," 0.30745571851730347,\n"," 0.30745550990104675,\n"," 0.3074553310871124,\n"," 0.30745500326156616,\n"," 0.307454913854599,\n"," 0.307454913854599,\n"," 0.307454913854599,\n"," 0.30745455622673035,\n"," 0.30745452642440796,\n"," 0.30745449662208557,\n"," 0.3074544668197632,\n"," 0.3074544370174408,\n"," 0.3074544370174408,\n"," 0.30745434761047363,\n"," 0.30745428800582886,\n"," 0.30745425820350647,\n"," 0.30745410919189453,\n"," 0.30745404958724976,\n"," 0.30745404958724976,\n"," 0.3074539303779602,\n"," 0.3074539005756378,\n"," 0.30745387077331543,\n"," 0.30745387077331543,\n"," 0.30745387077331543,\n"," 0.30745384097099304,\n"," 0.3074537217617035,\n"," 0.3074536919593811,\n"," 0.30745360255241394,\n"," 0.30745360255241394,\n"," 0.30745354294776917,\n"," 0.3074535131454468,\n"," 0.3074535131454468,\n"," 0.30745333433151245,\n"," 0.3074532449245453,\n"," 0.3074531853199005,\n"," 0.3074531853199005,\n"," 0.3074531853199005,\n"," 0.3074530363082886,\n"," 0.3074527978897095,\n"," 0.3074527680873871,\n"," 0.30745258927345276,\n"," 0.30745255947113037,\n"," 0.30745241045951843,\n"," 0.3074522614479065,\n"," 0.3074522614479065,\n"," 0.3074522018432617,\n"," 0.3074520230293274,\n"," 0.30745193362236023,\n"," 0.30745163559913635,\n"," 0.3074515163898468,\n"," 0.30745136737823486,\n"," 0.3074513077735901,\n"," 0.30745112895965576,\n"," 0.3074510991573334,\n"," 0.3074509799480438,\n"," 0.30745092034339905,\n"," 0.3074508607387543,\n"," 0.3074505031108856,\n"," 0.30745044350624084,\n"," 0.30745038390159607,\n"," 0.3074503540992737,\n"," 0.3074501156806946,\n"," 0.3074500858783722,\n"," 0.3074500858783722,\n"," 0.30744999647140503,\n"," 0.30744996666908264,\n"," 0.3074495494365692,\n"," 0.3074495196342468,\n"," 0.3074495196342468,\n"," 0.30744948983192444,\n"," 0.3074494004249573,\n"," 0.3074493408203125,\n"," 0.3074493110179901,\n"," 0.3074491322040558,\n"," 0.30744901299476624,\n"," 0.30744898319244385,\n"," 0.3074489235877991,\n"," 0.3074488639831543,\n"," 0.30744877457618713,\n"," 0.3074486255645752,\n"," 0.3074485659599304,\n"," 0.30744850635528564,\n"," 0.30744847655296326,\n"," 0.30744847655296326,\n"," 0.3074483275413513,\n"," 0.30744829773902893,\n"," 0.30744823813438416,\n"," 0.3074481785297394,\n"," 0.3074481785297394,\n"," 0.307448148727417,\n"," 0.3074480891227722,\n"," 0.30744805932044983,\n"," 0.30744799971580505,\n"," 0.3074479401111603,\n"," 0.3074478507041931,\n"," 0.3074478507041931,\n"," 0.30744779109954834,\n"," 0.30744776129722595,\n"," 0.30744776129722595,\n"," 0.30744773149490356,\n"," 0.3074475824832916,\n"," 0.30744752287864685,\n"," 0.30744749307632446,\n"," 0.3074474632740021,\n"," 0.3074474632740021,\n"," 0.3074474632740021,\n"," 0.3074474036693573,\n"," 0.3074473738670349,\n"," 0.30744731426239014,\n"," 0.30744731426239014,\n"," 0.307447224855423,\n"," 0.307447224855423,\n"," 0.307447224855423,\n"," 0.30744704604148865,\n"," 0.30744704604148865,\n"," 0.30744701623916626,\n"," 0.3074468970298767,\n"," 0.30744683742523193,\n"," 0.30744674801826477,\n"," 0.30744674801826477,\n"," 0.3074467182159424,\n"," 0.3074467182159424,\n"," 0.3074467182159424,\n"," 0.3074466586112976,\n"," 0.3074466288089752,\n"," 0.3074464797973633,\n"," 0.3074464499950409,\n"," 0.30744636058807373,\n"," 0.30744627118110657,\n"," 0.30744627118110657,\n"," 0.30744627118110657,\n"," 0.3074462413787842,\n"," 0.3074461817741394,\n"," 0.3074461817741394,\n"," 0.30744609236717224,\n"," 0.30744609236717224,\n"," 0.3074459135532379,\n"," 0.3074458837509155,\n"," 0.30744585394859314,\n"," 0.30744585394859314,\n"," 0.30744582414627075,\n"," 0.307445764541626,\n"," 0.307445764541626,\n"," 0.307445764541626,\n"," 0.3074456751346588,\n"," 0.3074455261230469,\n"," 0.3074454069137573,\n"," 0.30744537711143494,\n"," 0.3074452877044678,\n"," 0.3074451684951782,\n"," 0.30744513869285583,\n"," 0.3074449598789215,\n"," 0.30744481086730957,\n"," 0.3074447810649872,\n"," 0.3074447214603424,\n"," 0.3074444830417633,\n"," 0.3074444830417633,\n"," 0.30744439363479614,\n"," 0.30744439363479614,\n"," 0.30744439363479614,\n"," 0.30744439363479614,\n"," 0.30744436383247375,\n"," 0.30744436383247375,\n"," 0.30744418501853943,\n"," 0.30744415521621704,\n"," 0.30744415521621704,\n"," 0.30744415521621704,\n"," 0.3074440658092499,\n"," 0.3074440360069275,\n"," 0.3074440360069275,\n"," 0.3074440062046051,\n"," 0.3074438273906708,\n"," 0.3074437975883484,\n"," 0.3074437975883484,\n"," 0.307443767786026,\n"," 0.307443767786026,\n"," 0.30744367837905884,\n"," 0.3074433505535126,\n"," 0.3074433505535126,\n"," 0.3074433207511902,\n"," 0.30744317173957825,\n"," 0.30744314193725586,\n"," 0.3074430525302887,\n"," 0.3074429929256439,\n"," 0.30744287371635437,\n"," 0.30744287371635437,\n"," 0.3074427545070648,\n"," 0.30744272470474243,\n"," 0.30744269490242004,\n"," 0.30744263529777527,\n"," 0.3074426054954529,\n"," 0.3074425458908081,\n"," 0.3074425458908081,\n"," 0.30744242668151855,\n"," 0.3074423372745514,\n"," 0.3074423372745514,\n"," 0.3074422776699066,\n"," 0.30744215846061707,\n"," 0.3074421286582947,\n"," 0.3074420690536499,\n"," 0.3074420690536499,\n"," 0.3074420392513275,\n"," 0.3074420094490051,\n"," 0.3074420094490051,\n"," 0.3074420094490051,\n"," 0.30744192004203796,\n"," 0.307441771030426,\n"," 0.30744174122810364,\n"," 0.30744168162345886,\n"," 0.30744147300720215,\n"," 0.30744147300720215,\n"," 0.30744144320487976,\n"," 0.307441383600235,\n"," 0.3074413537979126,\n"," 0.3074413239955902,\n"," 0.3074412941932678,\n"," 0.30744126439094543,\n"," 0.30744126439094543,\n"," 0.30744117498397827,\n"," 0.30744117498397827,\n"," 0.3074411451816559,\n"," 0.30744093656539917,\n"," 0.3074409067630768,\n"," 0.30744078755378723,\n"," 0.30744078755378723,\n"," 0.30744075775146484,\n"," 0.30744054913520813,\n"," 0.30744054913520813,\n"," 0.30744045972824097,\n"," 0.30744045972824097,\n"," 0.30744045972824097,\n"," 0.30744045972824097,\n"," 0.3074403703212738,\n"," 0.30744031071662903,\n"," 0.30744031071662903,\n"," 0.30744028091430664,\n"," 0.30744022130966187,\n"," 0.3074401617050171,\n"," 0.3074401617050171,\n"," 0.3074401021003723,\n"," 0.3074400722980499,\n"," 0.30744001269340515,\n"," 0.307439923286438,\n"," 0.3074398636817932,\n"," 0.3074398338794708,\n"," 0.3074398338794708,\n"," 0.30743980407714844,\n"," 0.30743974447250366,\n"," 0.30743974447250366,\n"," 0.30743974447250366,\n"," 0.3074396550655365,\n"," 0.3074396252632141,\n"," 0.3074396252632141,\n"," 0.30743956565856934,\n"," 0.30743956565856934,\n"," 0.30743953585624695,\n"," 0.30743953585624695,\n"," 0.30743950605392456,\n"," 0.3074394762516022,\n"," 0.3074394464492798,\n"," 0.3074394464492798,\n"," 0.3074394464492798,\n"," 0.3074394166469574,\n"," 0.3074393570423126,\n"," 0.30743932723999023,\n"," 0.30743926763534546,\n"," 0.3074392080307007,\n"," 0.3074391782283783,\n"," 0.3074391484260559,\n"," 0.30743908882141113,\n"," 0.30743908882141113,\n"," 0.30743908882141113,\n"," 0.30743902921676636,\n"," 0.30743899941444397,\n"," 0.3074389100074768,\n"," 0.30743885040283203,\n"," 0.3074387013912201,\n"," 0.3074386417865753,\n"," 0.30743861198425293,\n"," 0.30743861198425293,\n"," 0.30743858218193054,\n"," 0.30743858218193054,\n"," 0.30743855237960815,\n"," 0.30743855237960815,\n"," 0.30743852257728577,\n"," 0.30743837356567383,\n"," 0.30743834376335144,\n"," 0.30743831396102905,\n"," 0.3074382245540619,\n"," 0.3074382245540619,\n"," 0.3074381649494171,\n"," 0.30743807554244995,\n"," 0.30743807554244995,\n"," 0.3074379861354828,\n"," 0.3074378967285156,\n"," 0.3074378967285156,\n"," 0.30743786692619324,\n"," 0.30743783712387085,\n"," 0.30743780732154846,\n"," 0.30743780732154846,\n"," 0.3074377477169037,\n"," 0.3074376881122589,\n"," 0.30743759870529175,\n"," 0.307437539100647,\n"," 0.307437539100647,\n"," 0.3074375092983246,\n"," 0.3074375092983246,\n"," 0.3074375092983246,\n"," 0.3074375092983246,\n"," 0.30743739008903503,\n"," 0.3074372708797455,\n"," 0.3074372112751007,\n"," 0.30743715167045593,\n"," 0.30743715167045593,\n"," 0.3074369728565216,\n"," 0.30743688344955444,\n"," 0.3074367642402649,\n"," 0.3074367642402649,\n"," 0.3074367046356201,\n"," 0.30743667483329773,\n"," 0.30743667483329773,\n"," 0.30743667483329773,\n"," 0.30743661522865295,\n"," 0.30743658542633057,\n"," 0.3074365556240082,\n"," 0.3074365258216858,\n"," 0.3074365258216858,\n"," 0.307436466217041,\n"," 0.30743643641471863,\n"," 0.30743640661239624,\n"," 0.30743637681007385,\n"," 0.30743637681007385,\n"," 0.30743637681007385,\n"," 0.3074363172054291,\n"," 0.3074362277984619,\n"," 0.3074361979961395,\n"," 0.30743613839149475,\n"," 0.30743613839149475,\n"," 0.30743613839149475,\n"," 0.30743613839149475,\n"," 0.30743610858917236,\n"," 0.3074360489845276,\n"," 0.3074360489845276,\n"," 0.3074359595775604,\n"," 0.3074359595775604,\n"," 0.3074359595775604,\n"," 0.30743589997291565,\n"," 0.30743589997291565,\n"," 0.3074358105659485,\n"," 0.30743566155433655,\n"," 0.30743566155433655,\n"," 0.30743566155433655,\n"," 0.30743563175201416,\n"," 0.3074355721473694,\n"," 0.3074355721473694,\n"," 0.3074355125427246,\n"," 0.3074354827404022,\n"," 0.30743545293807983,\n"," 0.30743542313575745,\n"," 0.30743542313575745,\n"," 0.30743536353111267,\n"," 0.3074353337287903,\n"," 0.3074353337287903,\n"," 0.3074353039264679,\n"," 0.3074352741241455,\n"," 0.30743521451950073,\n"," 0.30743521451950073,\n"," 0.30743512511253357,\n"," 0.3074350953102112,\n"," 0.307435005903244,\n"," 0.307435005903244,\n"," 0.30743497610092163,\n"," 0.30743497610092163,\n"," 0.3074348568916321,\n"," 0.3074348270893097,\n"," 0.3074348270893097,\n"," 0.3074347674846649,\n"," 0.30743473768234253,\n"," 0.30743470788002014,\n"," 0.30743470788002014,\n"," 0.30743470788002014,\n"," 0.307434618473053,\n"," 0.307434618473053,\n"," 0.307434618473053,\n"," 0.3074345886707306,\n"," 0.3074345588684082,\n"," 0.3074345588684082,\n"," 0.3074345588684082,\n"," 0.3074345290660858,\n"," 0.3074345290660858,\n"," 0.3074344992637634,\n"," 0.30743446946144104,\n"," 0.30743446946144104,\n"," 0.3074343800544739,\n"," 0.3074343502521515,\n"," 0.3074343502521515,\n"," 0.3074343502521515,\n"," 0.3074342906475067,\n"," 0.3074342906475067,\n"," 0.3074342608451843,\n"," 0.30743420124053955,\n"," 0.30743420124053955,\n"," 0.30743420124053955,\n"," 0.3074341118335724,\n"," 0.30743408203125,\n"," 0.3074340522289276,\n"," 0.3074340522289276,\n"," 0.3074340224266052,\n"," 0.30743399262428284,\n"," 0.30743396282196045,\n"," 0.3074339032173157,\n"," 0.3074339032173157,\n"," 0.3074338436126709,\n"," 0.3074338138103485,\n"," 0.3074337840080261,\n"," 0.30743375420570374,\n"," 0.30743375420570374,\n"," 0.30743372440338135,\n"," 0.3074336349964142,\n"," 0.3074336051940918,\n"," 0.30743342638015747,\n"," 0.30743342638015747,\n"," 0.30743342638015747,\n"," 0.30743342638015747,\n"," 0.3074333667755127,\n"," 0.3074333369731903,\n"," 0.30743327736854553,\n"," 0.307433158159256,\n"," 0.307433158159256,\n"," 0.3074331283569336,\n"," 0.3074330687522888,\n"," 0.30743303894996643,\n"," 0.30743300914764404,\n"," 0.30743297934532166,\n"," 0.30743297934532166,\n"," 0.30743297934532166,\n"," 0.3074328899383545,\n"," 0.3074328899383545,\n"," 0.3074328899383545,\n"," 0.3074328601360321,\n"," 0.3074328303337097,\n"," 0.30743274092674255,\n"," 0.30743271112442017,\n"," 0.30743271112442017,\n"," 0.307432621717453,\n"," 0.307432621717453,\n"," 0.3074325919151306,\n"," 0.3074325621128082,\n"," 0.3074324429035187,\n"," 0.30743229389190674,\n"," 0.3074321448802948,\n"," 0.3074321448802948,\n"," 0.3074321150779724,\n"," 0.3074321150779724,\n"," 0.3074321150779724,\n"," 0.3074321150779724,\n"," 0.30743205547332764,\n"," 0.30743205547332764,\n"," 0.30743205547332764,\n"," 0.30743202567100525,\n"," 0.30743202567100525,\n"," 0.3074319362640381,\n"," 0.3074319362640381,\n"," 0.3074318766593933,\n"," 0.3074318766593933,\n"," 0.3074318468570709,\n"," 0.30743181705474854,\n"," 0.30743181705474854,\n"," 0.30743178725242615,\n"," 0.30743172764778137,\n"," 0.30743172764778137,\n"," 0.307431697845459,\n"," 0.3074316382408142,\n"," 0.30743157863616943,\n"," 0.30743157863616943,\n"," 0.30743157863616943,\n"," 0.30743157863616943,\n"," 0.30743157863616943,\n"," 0.30743151903152466,\n"," 0.30743151903152466,\n"," 0.30743151903152466,\n"," 0.30743151903152466,\n"," 0.30743151903152466,\n"," 0.30743148922920227,\n"," 0.30743148922920227,\n"," 0.3074314296245575,\n"," 0.3074314296245575,\n"," 0.3074313998222351,\n"," 0.3074313700199127,\n"," 0.30743131041526794,\n"," 0.30743128061294556,\n"," 0.30743125081062317,\n"," 0.30743125081062317,\n"," 0.30743125081062317,\n"," 0.3074311912059784,\n"," 0.3074311912059784,\n"," 0.3074311912059784,\n"," 0.307431161403656,\n"," 0.3074311316013336,\n"," 0.30743110179901123,\n"," 0.30743107199668884,\n"," 0.30743104219436646,\n"," 0.30743104219436646,\n"," 0.30743104219436646,\n"," 0.3074309825897217,\n"," 0.3074309527873993,\n"," 0.3074309229850769,\n"," 0.3074309229850769,\n"," 0.3074308931827545,\n"," 0.3074308931827545,\n"," 0.30743083357810974,\n"," 0.3074307441711426,\n"," 0.3074307441711426,\n"," 0.3074307441711426,\n"," 0.307430624961853,\n"," 0.30743059515953064,\n"," 0.30743053555488586,\n"," 0.3074304759502411,\n"," 0.3074304461479187,\n"," 0.3074304163455963,\n"," 0.30743032693862915,\n"," 0.30743032693862915,\n"," 0.30743029713630676,\n"," 0.30743029713630676,\n"," 0.30743029713630676,\n"," 0.30743029713630676,\n"," 0.3074301779270172,\n"," 0.3074301481246948,\n"," 0.30743011832237244,\n"," 0.30743011832237244,\n"," 0.30743011832237244,\n"," 0.30743008852005005,\n"," 0.30743005871772766,\n"," 0.3074300289154053,\n"," 0.3074299395084381,\n"," 0.3074299097061157,\n"," 0.3074299097061157,\n"," 0.30742987990379333,\n"," 0.30742982029914856,\n"," 0.3074297606945038,\n"," 0.3074297308921814,\n"," 0.307429701089859,\n"," 0.3074296712875366,\n"," 0.30742964148521423,\n"," 0.30742958188056946,\n"," 0.3074294924736023,\n"," 0.3074294924736023,\n"," 0.3074294626712799,\n"," 0.3074294328689575,\n"," 0.30742937326431274,\n"," 0.3074292540550232,\n"," 0.3074292540550232,\n"," 0.3074292540550232,\n"," 0.3074291944503784,\n"," 0.3074291944503784,\n"," 0.30742916464805603,\n"," 0.30742907524108887,\n"," 0.30742907524108887,\n"," 0.3074290454387665,\n"," 0.3074290156364441,\n"," 0.3074290156364441,\n"," 0.3074289858341217,\n"," 0.3074289560317993,\n"," 0.3074289560317993,\n"," 0.30742892622947693,\n"," 0.30742892622947693,\n"," 0.30742889642715454,\n"," 0.30742889642715454,\n"," 0.30742883682250977,\n"," 0.3074288070201874,\n"," 0.307428777217865,\n"," 0.3074287474155426,\n"," 0.30742865800857544,\n"," 0.30742862820625305,\n"," 0.30742859840393066,\n"," 0.30742859840393066,\n"," 0.30742859840393066,\n"," 0.30742859840393066,\n"," 0.3074285686016083,\n"," 0.3074285387992859,\n"," 0.3074285089969635,\n"," 0.3074285089969635,\n"," 0.3074284791946411,\n"," 0.3074284791946411,\n"," 0.3074284493923187,\n"," 0.30742841958999634,\n"," 0.30742841958999634,\n"," 0.30742838978767395,\n"," 0.30742835998535156,\n"," 0.307428240776062,\n"," 0.30742818117141724,\n"," 0.30742818117141724,\n"," 0.30742818117141724,\n"," 0.30742815136909485,\n"," 0.30742815136909485,\n"," 0.30742812156677246,\n"," 0.30742812156677246,\n"," 0.3074280321598053,\n"," 0.3074280023574829,\n"," 0.30742791295051575,\n"," 0.30742791295051575,\n"," 0.30742788314819336,\n"," 0.30742788314819336,\n"," 0.30742788314819336,\n"," 0.30742788314819336,\n"," 0.30742788314819336,\n"," 0.3074278235435486,\n"," 0.3074278235435486,\n"," 0.3074277341365814,\n"," 0.30742770433425903,\n"," 0.30742770433425903,\n"," 0.30742767453193665,\n"," 0.30742761492729187,\n"," 0.30742761492729187,\n"," 0.3074275553226471,\n"," 0.3074275553226471,\n"," 0.30742746591567993,\n"," 0.30742746591567993,\n"," 0.30742743611335754,\n"," 0.30742740631103516,\n"," 0.30742737650871277,\n"," 0.3074273467063904,\n"," 0.3074273467063904,\n"," 0.3074273467063904,\n"," 0.307427316904068,\n"," 0.3074272871017456,\n"," 0.30742719769477844,\n"," 0.30742716789245605,\n"," 0.30742716789245605,\n"," 0.30742716789245605,\n"," 0.3074271082878113,\n"," 0.3074270784854889,\n"," 0.3074270784854889,\n"," 0.3074270784854889,\n"," 0.3074270188808441,\n"," 0.3074270188808441,\n"," 0.30742698907852173,\n"," 0.30742695927619934,\n"," 0.30742692947387695,\n"," 0.30742689967155457,\n"," 0.30742689967155457,\n"," 0.3074268698692322,\n"," 0.30742672085762024,\n"," 0.30742672085762024,\n"," 0.30742672085762024,\n"," 0.30742669105529785,\n"," 0.30742666125297546,\n"," 0.3074266314506531,\n"," 0.3074265718460083,\n"," 0.3074265420436859,\n"," 0.3074265420436859,\n"," 0.3074265122413635,\n"," 0.3074265122413635,\n"," 0.30742648243904114,\n"," 0.30742642283439636,\n"," 0.30742642283439636,\n"," 0.30742642283439636,\n"," 0.3074263334274292,\n"," 0.30742624402046204,\n"," 0.30742621421813965,\n"," 0.30742618441581726,\n"," 0.3074261546134949,\n"," 0.3074261248111725,\n"," 0.3074260354042053,\n"," 0.30742600560188293,\n"," 0.30742597579956055,\n"," 0.30742594599723816,\n"," 0.30742594599723816,\n"," 0.30742591619491577,\n"," 0.30742591619491577,\n"," 0.307425856590271,\n"," 0.307425856590271,\n"," 0.3074258267879486,\n"," 0.3074258267879486,\n"," 0.3074258267879486,\n"," 0.30742576718330383,\n"," 0.30742570757865906,\n"," 0.30742567777633667,\n"," 0.30742567777633667,\n"," 0.30742567777633667,\n"," 0.3074256479740143,\n"," 0.3074256479740143,\n"," 0.3074256181716919,\n"," 0.3074256181716919,\n"," 0.3074255883693695,\n"," 0.3074255585670471,\n"," 0.3074255585670471,\n"," 0.3074255585670471,\n"," 0.30742549896240234,\n"," 0.30742546916007996,\n"," 0.30742546916007996,\n"," 0.30742546916007996,\n"," 0.3074254095554352,\n"," 0.3074254095554352,\n"," 0.3074254095554352,\n"," 0.3074254095554352,\n"," 0.307425320148468,\n"," 0.307425320148468,\n"," 0.307425320148468,\n"," 0.30742529034614563,\n"," 0.3074251711368561,\n"," 0.3074251413345337,\n"," 0.3074250817298889,\n"," 0.30742505192756653,\n"," 0.30742505192756653,\n"," 0.30742505192756653,\n"," 0.30742499232292175,\n"," 0.30742499232292175,\n"," 0.30742499232292175,\n"," 0.307424932718277,\n"," 0.307424932718277,\n"," 0.307424932718277,\n"," 0.3074249029159546,\n"," 0.3074249029159546,\n"," 0.3074248433113098,\n"," 0.3074248433113098,\n"," 0.30742475390434265,\n"," 0.30742475390434265,\n"," 0.3074246644973755,\n"," 0.3074246346950531,\n"," 0.3074245750904083,\n"," 0.3074245750904083,\n"," 0.3074245750904083,\n"," 0.30742454528808594,\n"," 0.30742448568344116,\n"," 0.30742448568344116,\n"," 0.3074244260787964,\n"," 0.307424396276474,\n"," 0.3074243664741516,\n"," 0.3074243366718292,\n"," 0.30742430686950684,\n"," 0.30742430686950684,\n"," 0.30742430686950684,\n"," 0.30742430686950684,\n"," 0.3074242174625397,\n"," 0.3074241280555725,\n"," 0.3074241280555725,\n"," 0.3074240982532501,\n"," 0.30742400884628296,\n"," 0.30742400884628296,\n"," 0.3074239492416382,\n"," 0.3074239492416382,\n"," 0.307423859834671,\n"," 0.307423859834671,\n"," 0.30742383003234863,\n"," 0.30742377042770386,\n"," 0.30742377042770386,\n"," 0.30742374062538147,\n"," 0.3074236214160919,\n"," 0.30742359161376953,\n"," 0.30742350220680237,\n"," 0.30742350220680237,\n"," 0.30742350220680237,\n"," 0.30742347240448,\n"," 0.30742332339286804,\n"," 0.30742332339286804,\n"," 0.30742329359054565,\n"," 0.3074232339859009,\n"," 0.3074232339859009,\n"," 0.3074231743812561,\n"," 0.3074231445789337,\n"," 0.3074231445789337,\n"," 0.30742311477661133,\n"," 0.30742308497428894,\n"," 0.30742308497428894,\n"," 0.30742305517196655,\n"," 0.3074229955673218,\n"," 0.3074229657649994,\n"," 0.3074229061603546,\n"," 0.3074229061603546,\n"," 0.3074229061603546,\n"," 0.3074229061603546,\n"," 0.3074229061603546,\n"," 0.30742281675338745,\n"," 0.30742281675338745,\n"," 0.3074227273464203,\n"," 0.3074227273464203,\n"," 0.3074227273464203,\n"," 0.3074226379394531,\n"," 0.3074226379394531,\n"," 0.3074226379394531,\n"," 0.30742260813713074,\n"," 0.30742257833480835,\n"," 0.30742257833480835,\n"," 0.30742254853248596,\n"," 0.30742254853248596,\n"," 0.30742254853248596,\n"," 0.3074225187301636,\n"," 0.3074225187301636,\n"," 0.3074225187301636,\n"," 0.3074225187301636,\n"," 0.3074224889278412,\n"," 0.3074224293231964,\n"," 0.3074224293231964,\n"," 0.307422399520874,\n"," 0.30742236971855164,\n"," 0.30742233991622925,\n"," 0.30742233991622925,\n"," 0.3074222803115845,\n"," 0.3074222803115845,\n"," 0.3074222803115845,\n"," 0.3074222803115845,\n"," 0.3074221611022949,\n"," 0.3074221611022949,\n"," 0.30742204189300537,\n"," 0.30742204189300537,\n"," 0.3074219524860382,\n"," 0.3074219524860382,\n"," 0.3074219524860382,\n"," 0.3074219226837158,\n"," 0.30742189288139343,\n"," 0.30742186307907104,\n"," 0.30742186307907104,\n"," 0.30742183327674866,\n"," 0.3074217736721039,\n"," 0.3074216842651367,\n"," 0.30742165446281433,\n"," 0.30742162466049194,\n"," 0.30742162466049194,\n"," 0.30742162466049194,\n"," 0.30742162466049194,\n"," 0.30742159485816956,\n"," 0.30742159485816956,\n"," 0.30742159485816956,\n"," 0.3074215352535248,\n"," 0.3074215352535248,\n"," 0.3074215352535248,\n"," 0.3074215054512024,\n"," 0.3074215054512024,\n"," 0.3074214458465576,\n"," 0.3074214458465576,\n"," 0.3074214458465576,\n"," 0.3074214458465576,\n"," 0.30742138624191284,\n"," 0.30742138624191284,\n"," 0.30742135643959045,\n"," 0.3074212968349457,\n"," 0.3074212670326233,\n"," 0.3074212372303009,\n"," 0.3074212372303009,\n"," 0.3074212074279785,\n"," 0.30742117762565613,\n"," 0.30742114782333374,\n"," 0.30742114782333374,\n"," 0.30742114782333374,\n"," 0.3074210584163666,\n"," 0.3074210584163666,\n"," 0.3074209988117218,\n"," 0.3074209988117218,\n"," 0.3074209988117218,\n"," ...]"]},"metadata":{},"execution_count":47}]},{"cell_type":"code","source":[],"metadata":{"id":"9hf_zNmohUkq"},"execution_count":null,"outputs":[]}]}